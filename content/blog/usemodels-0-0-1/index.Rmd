---
output: hugodown::hugo_document

slug: usemodels-0-0-1
title: usemodels 0-0-1
date: 2020-09-29
author: Max Kuhn
description: >
    The new usemodels R package is a helpful way to automatically generate 
    tidymodels code. 

photo:
  url: https://unsplash.com/photos/0TH1H1rq_eY
  author: Neven Krcmarek

categories: [package] 
tags: [tidymodels,parsnip,recipes,tune]
---

```{r startup, include = FALSE}
library(tidymodels)
library(usemodels)
options(width = 90)
```

We're very excited to announce the first release of the [usemodels](https://usemodels.tidymodels.org/) package. The tidymodels packages are designed to provide modeling functions that are highly flexible and modular. This is powerful but sometimes a template or skeleton showing how to start is helpful. The usemodels package creates templates for tidymodels analyses so you don't have to write as much new code. 

You can install it from CRAN with:

```{r, eval = FALSE}
install.packages("usemodels")
```

This blog post will show how to use the package. 

Let's start with creating a glmnet linear regression model for the `mtcars` data using tidymodels. This model is usually tuned over the amount and type of regularization. In tidymodels, there are a few intermediate steps for a glmnet model: 

 * Create a [parsnip model object](https://www.tmwr.org/models.html) and define the tuning parameters that we want to optimize. 
 
 * [Create a recipe](https://www.tmwr.org/recipes.html) that, at minimum, centers and scales the predictors. For some data sets, we also need to create dummy variables from any factor-encoded predictor columns. 
 
 * Define a [resampling scheme](https://www.tmwr.org/resampling.html) for our data. 
 
 * Choose a function from the [tune package](https://tune.tidymodels.org/), such as `tune_grid()`, to optimize the parameters. For grid search, we'll also need a grid of candidate parameter values (or let the function choose one for us). 
 
We recognize that this might be more code than you would have had to write compared to a package like `caret`. However, the tidymodels ecosystem enables a wider variety of modeling techniques and has slightly higher complexity.

usemodels automates much of this code infrastructure. For example: 

```{r glmnet-basic, results = 'hide', prompt = TRUE}
library(usemodels)
use_glmnet(mpg ~ ., data = mtcars)
```
 
which produces the terminal output:

```{r glmnet-basic-code, echo = FALSE, comment=""}
library(usemodels)
use_glmnet(mpg ~ ., data = mtcars)
```
 
This can be copied to the source window and edited. Some notes: 

 * For this model, it is possible to prescribe a default grid of candidate tuning parameter values that  work well about 90% of the time. For other models, the grid might be data-driven. In these cases, our functions `tune` functions can estimate an appropriate grid. 
 
 * The extra recipes steps are the [recommend preprocessing](https://www.tmwr.org/pre-proc-table.html) for this model. Since this varies from model-to-model, the recipe template will contain the minimal required steps. Your data might require additional operations. 
 
 * One thing that _should not be automated_ is the choice of resampling method. The code templates require the user to choose the `rsample` function that is appropriate. 
 
In case you are unfamiliar with the model and its preprocessing needs, a `verbose` option prints comments that explain _why_ some steps are included. For the glmnet model, the comments added to the recipe state: 

> Regularization methods sum up functions of the model slope coefficients. Because of this, the predictor variables should be on the same scale. Before centering and scaling the numeric predictors, any predictors with a single unique value are filtered out. 

Let's look at another example. The `ad_data` data set in the modeldata package has rows for 333 patients with a factor outcome for their level of cognitive impairment (e.g., Alzheimer's disease). There are also a categorical predictor in the data, the Apolipoprotein E genotype, which has six levels. Let's suppose the `Genotype` column was encoded as character (instead of being a factor). This might be a problem if the resampling method samples out a level from the data used to fit the model.

Let's use a boosted tree model with the xgboost package and change the default prefix for the objects:

```{r ad-data, results = "hide", prompt = TRUE}
library(tidymodels)
data(ad_data)

ad_data$Genotype <- as.character(ad_data$Genotype)

use_xgboost(Class ~ ., data = ad_data, prefix = "impairment")
```
```{r ad-data-code, echo = FALSE, comment=""}
use_xgboost(Class ~ ., data = ad_data, prefix = "impairment")
```

Notice that the line

```
step_string2factor(one_of(Genotype)) 
```

is included in the recipe along with a step to generate one-hot encoded dummy variables. xgboost is one of the few tree ensemble implementations that requires the user to create dummy variables. This step is only added to the template when it is required for that model.

Also, for this particular model, we recommend using a [space-filling design](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=space+filling+design+of+experiments) for the grid but the user must choose the number of grid points. 

The current set of templates included in the inaugural version of the package are: 

```{r}
ls("package:usemodels", pattern = "^use_")
```

We'll likely add more but please file [an issue](https://github.com/tidymodels/usemodels/issues) if there are any that you see as a priority. 
