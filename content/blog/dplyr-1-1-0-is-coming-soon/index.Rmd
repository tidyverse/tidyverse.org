---
output: hugodown::hugo_document
slug: dplyr-1-1-0-is-coming-soon
title: dplyr 1.1.0 is coming soon
date: 2022-11-22
author: Davis Vaughan
description: >
    dplyr 1.1.0 is coming soon! This post introduces some of the exciting new
    features coming in 1.1.0, and includes a call-for-feedback as we finalize
    the release.
photo:
  url: https://unsplash.com/photos/aId-xYRTlEc
  author: Markus Winkler
categories: [package] 
tags: [dplyr]
editor_options: 
  chunk_output_type: console
---

[dplyr](https://dplyr.tidyverse.org/dev/) 1.1.0 is coming soon!
This is an exciting release for dplyr, incorporating a number of features that have been in flight for years, including:

-   An inline alternative to `group_by()` that implements temporary grouping

-   New join features, such as non-equi joins

-   `arrange()` improvements with character vectors

-   `morph()`, a generalization of `summarise()`

-   Rewrites of "vector" functions using vctrs, including exciting updates to `case_when()`

This pre-release blog post will discuss these new features in more detail.
By releasing this post before 1.1.0 is sent to CRAN, we're hoping to get your feedback to catch any potential problems that we've missed!
If you do find a bug, or have general feedback about the new features, we welcome discussion on the [dplyr issues page](https://github.com/tidyverse/dplyr/issues).

You can see a full list of changes in the [release notes](https://dplyr.tidyverse.org/dev/news/index.html).
There are many additional improvements that couldn't fit in a single blog post!

dplyr 1.1.0 is not on CRAN yet, but you can install the development version from GitHub with:

```{r, eval = FALSE}
pak::pak("tidyverse/dplyr")
```

The development version is mostly stable, but is still subject to minor changes before the official release.
We don't encourage relying on it for production usage, but we would love for you to try out these new features.

```{r setup, warning=FALSE, message=FALSE}
library(dplyr)
library(clock)
set.seed(12345)
```

## Temporary grouping with `.by`

Verbs that work "by group," such as `mutate()`, `summarise()`, `filter()`, and `slice()`, have gained an experimental new argument, `.by`, which allows for inline and temporary grouping.
Grouping radically affects the computation of the dplyr verb you use it with, and one of the goals of `.by` is to allow you to place that grouping specification alongside the code that actually uses it.
As an added benefit, with `.by` you no longer need to remember to `ungroup()` after `summarise()`, and `summarise()` won't ever message you about how it's handling the groups!

This feature was inspired by [data.table](https://cran.r-project.org/package=data.table), which has always used per-operation grouping.

We'll explore `.by` with this `expenses` dataset, containing various `cost`s tracked across `id` and `region`.

```{r}
expenses <- tibble(
  id = c(1, 2, 1, 3, 1, 2, 3),
  region = c("A", "A", "A", "B", "B", "A", "A"),
  cost = c(25, 20, 19, 12, 9, 6, 6)
)
expenses
```

If I were to ask you to compute the average `cost` per `region`, you'd probably write something like:

```{r}
expenses |>
  group_by(region) |>
  summarise(cost = mean(cost))
```

With `.by`, you can now write:

```{r}
expenses |>
  summarise(cost = mean(cost), .by = region)
```

These two particular results look the same, but the behavior of `.by` diverges from `group_by()` when multiple group columns are involved:

```{r}
expenses |>
  group_by(id, region) |>
  summarise(cost = mean(cost))
```

```{r}
expenses |>
  summarise(cost = mean(cost), .by = c(id, region))
```

Usage of `.by` always results in an ungrouped data frame, regardless of the number of group columns involved.

You might also recognize that these results aren't returned in exactly the same order.
`group_by()` always sorts the grouping keys in ascending order, but `.by` retains the original ordering found in the data.
If you need ordered summaries with `.by`, we recommend calling `arrange()` explicitly before or after summarizing.

While here we've focused on using `.by` with `summarise()`, it also works with other verbs, like `mutate()` and `slice_max()`:

```{r}
expenses |>
  mutate(mean = mean(cost), .by = region)

expenses |>
  slice_max(cost, by = region)
```

`group_by()` won't ever disappear, but we are having a lot of fun writing new code with `.by`, and we think you will too.

## Join improvements

All of the join functions in dplyr, such as `left_join()`, now accept a flexible join specification created through the new `join_by()` helper.
`join_by()` allows you to specify your join conditions as expressions rather than as named character vectors.

```{r}
join_by(x_id == y_id, region)
```

This join specification matches `x_id` in the left-hand data frame with `y_id` in the right-hand one, and also matches between a commonly named `region` column, computing the following equi-join:

```{r}
df1 <- tibble(x_id = c(1, 2, 2), region = c("A", "B", "A"), x = c(5, 10, 4))
df2 <- tibble(y_id = c(2, 1, 2), region = c("A", "A", "C"), y = c(12, 8, 7))

df1

df2

df1 |>
  left_join(df2, join_by(x_id == y_id, region))
```

### Non-equi joins

Allowing expressions in `join_by()` opens up a whole new world of joins in dplyr known as *non-equi joins*.
As the name somewhat implies, these are joins that involve binary conditions other than equality.
There are 4 particularly useful types of non-equi joins:

-   **Cross joins** match every pair of rows and were already supported in dplyr.

-   **Inequality joins** match using `>`, `>=`, `<`, or `<=` instead of `==`.

-   **Rolling joins** are based on inequality joins, but only find the closest match.

-   **Overlap joins** are also based on inequality joins, but are specialized for working with ranges.

Non-equi joins were requested back in 2016, and were the highest requested dplyr feature at the time they were finally implemented, with over [147 thumbs up](https://github.com/tidyverse/dplyr/issues/2240)!
data.table has had support for non-equi joins for many years, and their implementation greatly inspired the one used in dplyr.

To demonstrate the different types of non-equi joins, imagine that you are in charge of the party planning committee for your office.
Unfortunately, you only get to have one party per quarter, but it is your job to ensure that every employee is assigned to a single party.
Upper management has provided the following 4 party dates:

```{r}
parties <- tibble(
  q = 1:4,
  party = date_parse(c("2022-01-10", "2022-04-04", "2022-07-11", "2022-10-03"))
)

parties
```

With this set of employees:

```{r}
employees <- tibble(
  name = wakefield::name(100),
  birthday = date_parse("2022-01-01") + (sample(365, 100, replace = TRUE) - 1)
)

employees
```

One way to start approaching this problem is to look for the party that happened directly before each birthday.
You can do this with an inequality join:

```{r}
employees |>
  left_join(parties, join_by(birthday >= party))
```

This looks like a good start, but we've assigned people with birthdays later in the year to multiple parties.
We can restrict this to only the party that is *closest* to the employee's birthday by using a rolling join.
Rolling joins are activated by wrapping an inequality in `closest()`.

```{r}
closest <- employees |>
  left_join(parties, join_by(closest(birthday >= party)))

closest
```

This is close to what we want, but isn't *quite* right.
It turns out that poor Della hasn't been assigned to a party.

```{r}
filter(closest, is.na(party))
```

This is because their birthday occurred before the first party date, `2022-01-10`, so there wasn't any "previous party" to match them to.
To fix this, it's better to be explicit about the quarter start/end dates when performing the join:

```{r}
# Some helpers from {clock}
quarter_start <- function(x) {
  x <- as_year_quarter_day(x)
  x <- calendar_start(x, "quarter")
  as_date(x)
}
quarter_end <- function(x) {
  x <- as_year_quarter_day(x)
  x <- calendar_end(x, "quarter")
  as.Date(x)
}

parties <- parties |>
  mutate(start = quarter_start(party), end = quarter_end(party))

parties
```

Now that we have 4 distinct *ranges* of dates to work with, we'll use an overlap join to figure out which range each birthday fell `between()`.
Since we know that each birthday should be matched to exactly one party, we'll also take this chance to set two of the new arguments to all of the join functions:

-   `unmatched`, which can optionally error if an unmatched birthday exists

-   `multiple`, which can optionally error if a birthday matches multiple parties

```{r}
employees |>
  left_join(
    parties, 
    join_by(between(birthday, start, end)),
    unmatched = "error",
    multiple = "error"
  )
```

We consider `unmatched` and `multiple` to be two important "quality control" arguments to help you enforce constraints on the join procedure.

### Multiple matches

Speaking of `multiple`, we've also given this argument an important default.
When doing data analysis with equi-joins, it is often surprising when a join returns more rows than were present in the left-hand side table.

```{r}
df1

df2 <- tibble(y_id = c(1, 2, 1, 2), region = c("A", "B", "A", "A"), y = c(9, 10, 12, 4))
df2

df1 |>
  left_join(df2, join_by(x_id == y_id, region))
```

In this case, row 1 of `df1` matched both rows `1` and `3` of `df2`, so the output has 4 rows rather than `df1`'s 3.
While this is standard SQL behavior, community feedback has shown that many people don't expect this, and a number of people were horrified to learn that this was even possible!
Because of this, we've made this case a warning by default, which you can silence with `multiple = "all"`.

## `arrange()` improvements with character vectors

`arrange()` now uses a new custom backend for generating the ordering.
This generally improves performance, but it is especially apparent with character vectors.

```{r}
# 10,000 random strings, sampled up to 1,000,000 rows
dictionary <- stringi::stri_rand_strings(10000, length = 10, pattern = "[a-z]")
str <- tibble(x = sample(dictionary, size = 1e6, replace = TRUE))
str
```

```{r, eval=FALSE}
# dplyr 1.0.10
bench::mark(arrange(str, x), iterations = 100)
#> # A tibble: 1 × 6
#>   expression          min   median `itr/sec` mem_alloc `gc/sec`
#>   <bch:expr>     <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>
#> 1 arrange(str, x)   4.38s    4.89s     0.204    12.7MB    0.148

# dplyr 1.1.0
bench::mark(arrange(str, x), iterations = 100)
#> # A tibble: 1 × 6
#>   expression          min   median `itr/sec` mem_alloc `gc/sec`
#>   <bch:expr>     <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>
#> 1 arrange(str, x)  42.3ms   46.6ms      20.8    22.4MB     46.0
```

For those keeping score, that is a 100x improvement!
Now, I'll be honest, I'm being a bit tricky here.
The new backend for `arrange()` comes with a meaningful change in behavior - it now sorts character strings in the C locale by default, rather than in the much slower system locale (American English, for me).
We made this change for two main reasons:

-   Much faster performance by default

-   Improved reproducibility across R sessions, where different computers might use different system locales

For English users, we expect this change to have fairly minimal impact.
The largest difference in ordering between the C and American English locales has to do with capitalization.
In the C locale, uppercase letters are always placed before *any* lowercase letters.
In the American English locale, uppercase letters are placed directly after their lowercase equivalent.

```{r}
df <- tibble(x = c("a", "B", "A", "b"))

arrange(df, x)
```

If you do need to order with a specific locale, you can specify the new `.locale` argument, which takes a locale identifier string, just like `stringr::str_sort()`.

```{r}
arrange(df, x, .locale = "en")
```

To use this optional `.locale` feature, you must have the stringi package installed, but you likely already do because it is installed with the tidyverse by default.

It is also worth noting that using `.locale` is still much faster than relying on the system locale.

```{r, eval=FALSE}
# Compare with ~5 seconds above with dplyr 1.0.10

bench::mark(arrange(str, x, .locale = "en"), iterations = 100)
#> # A tibble: 1 × 6
#>   expression                           min median `itr/sec` mem_alloc
#>   <bch:expr>                      <bch:tm> <bch:>     <dbl> <bch:byt>
#> 1 arrange(str, x, .locale = "en")    377ms  430ms      2.21    27.9MB
#> # … with 1 more variable: `gc/sec` <dbl>
```

For non-English Latin script languages, such as Spanish, you may see more of a change, as characters such as `ñ` are ordered after `z` rather than before `n` in the C locale:

```{r}
df <- tibble(x = c("\u00F1", "n", "z"))
df

arrange(df, x)

arrange(df, x, .locale = "es")
```

We are optimistic that this change is an overall net positive.
We anticipate that many users use `arrange()` to simply group similar looking observations together, and we expect that the main places you'll need to care about localized ordering are the few places when you are generating human readable output, such as a table or a chart, at which point you might consider using `.locale`.

If you are having trouble converting an existing script over to the new behavior, you can set the temporary global option `options(dplyr.legacy_locale = TRUE)`, which will revert to the pre-1.1.0 behavior of using the system locale.
We expect to remove this option in a future release.

To learn more low-level details about this change, you can read our [tidyup](https://github.com/tidyverse/tidyups/blob/main/003-dplyr-radix-ordering.md).

## `morph()`, a generalization of `summarise()`

In dplyr 1.0.0, we introduced a powerful new feature: `summarise()` could return per-group results of any length, rather than just length 1.
For example:

```{r, include=FALSE}
options(lifecycle_verbosity = "quiet")
```

```{r}
table <- c("a", "b", "d", "f")

df <- tibble(
  g = c(1, 1, 1, 2, 2, 2, 2),
  x = c("e", "a", "b", "c", "f", "d", "a")
)

df |>
  summarise(x = intersect(x, table), .by = g)
```

```{r, include=FALSE}
options(lifecycle_verbosity = NULL)
```

While extremely powerful, community feedback has raised the valid concern that allowing `summarise()` to return any number of rows per group:

-   Increases the chance for accidental bugs

-   Is against the spirit of a "summary," which implies 1 row per group

-   Makes translation to dbplyr very difficult

We agree!
In response to this, we've decided to walk back that change to `summarise()`, which will now throw a warning when either 0 or \>1 rows are returned per group:

```{r, include=FALSE}
options(lifecycle_verbosity = "warning")
```

```{r}
df |>
  summarise(x = intersect(x, table), .by = g)
```

```{r, include=FALSE}
options(lifecycle_verbosity = NULL)
```

That said, we still believe that this is a powerful tool, so we've moved these features to a new verb, `morph()`.
Think of `morph()` as a generic tool for "doing something to each group," with no restrictions on the number of rows returned per group.

```{r}
df |>
  morph(x = intersect(x, table), .by = g)
```

One big difference between `summarise()` and `morph()` is that `morph()` always returns an ungrouped data frame, even if the input was a grouped data frame with multiple groups.
This simplifies `morph()` immensely, as it doesn't need to inherit the `.groups` argument of `summarise()`, and never emits any messages.

We expect that you'll continue to use `summarise()` much more often than `morph()`, but if you ever find yourself applying a function to each group that returns an arbitrary number of rows, `morph()` should be your go-to tool!

## Vector function rewrites

We've taken a careful look at each of the "vector" functions in dplyr, such as `case_when()`, `dense_rank()`, and `between()`, and have rewritten them to use [vctrs](https://vctrs.r-lib.org/).
In general, this means that each of these functions can now be used on more generic data types, is often faster, and produces more consistent error messages.

### `case_when()`

The switch to vctrs has allowed us to relax the strict typing rules that were previously being used on the right-hand side inputs of `case_when()`.
Practically, this means that you no longer need to use typed missing values (like `NA_integer_` or `NA_character_`)!

```{r}
x <- c(1, 10, 12, -5, 6, -2, 0)

case_when(
  x >= 10 ~ "large",
  x >= 0 ~ "small",
  x < 0 ~ NA
)
```

We've also added an explicit `.default` argument that controls what value will be returned when all of the left-hand side conditions return either `FALSE` or `NA`.
We vastly prefer this over the previous convention of using `TRUE ~`.
We could rewrite our previous example using `.default` for the fall-through condition:

```{r}
case_when(
  x >= 10 ~ "large",
  x >= 0 ~ "small",
  .default = NA
)

case_when(
  x >= 10 ~ "large",
  x >= 0 ~ "small",
  .default = "invalid"
)
```

`if_else()` has received the same updates as `case_when()`, meaning that you no longer need to use typed missing values there either.

### `case_match()`

We've added a new cousin to `case_when()`, named `case_match()`.
While `case_when()` is like a vectorized `if else`, `case_match()` is more of a vectorized `switch()`.
Rather than supplying logical vectors on the left-hand side, you supply values to match against:

```{r}
x <- c("a", "b", "a", "d", "b", NA, "c", "e", "f")

case_match(
  x,
  c("a", "b") ~ "low",
  c("c", "d", "e") ~ "high"
)
```

Like with `case_when()`, `case_match()` defaults to returning a missing value for any unmatched element in `x`, but you can override this with `.default`:

```{r}
case_match(
  x,
  c("a", "b") ~ "low",
  c("c", "d", "e") ~ "high",
  .default = "unknown"
)
```

Since `.default` is vectorized, it is often useful to set it to the original vector, which let's you replace only the matched values, leaving everything else as is:

```{r}
case_match(
  x,
  c("a", "b") ~ "ab",
  c("c", "d", "e") ~ "cde",
  .default = x
)
```

### `consecutive_id()`

Last, but certainly not least, we've added a new function for generating consecutive groups, `consecutive_id()`.
This was inspired by `data.table::rleid()`.
Hadley recently used this to tidy up the transcript of our company meeting, and the idea looked something like this:

```{r}
transcript <- tribble(
  ~name, ~text,
  "Hadley", "I'll never learn Python.",
  "Davis", "But aren't you speaking at PyCon?",
  "Hadley", "So?",
  "Hadley", "That doesn't influence my decision.",
  "Hadley", "I'm not budging!",
  "Mara", "Typical, Hadley. Stubborn as always.",
  "Davis", "Fair enough!",
  "Davis", "Let's move on."
)
```

Ideally, we'd be able to flatten this into individual rows containing all of the dialogue between when a person started and stopped talking.
But we can't just group on `name`, as that would group the 1st line with lines 3-5.
Instead, we need to create a new *consecutive* grouping variable based on `name`:

```{r}
transcript <- transcript |>
  mutate(id = consecutive_id(name))

transcript
```

That gives us something to group on in our `summarise()` call.
For convenience, we'll also group on `name` so it gets carried through, but it won't affect our groupings.

```{r}
transcript |>
  summarise(
    text = stringr::str_flatten(text, collapse = " "), 
    .by = c(id, name)
  )
```
