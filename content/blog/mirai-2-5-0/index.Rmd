---
output: hugodown::hugo_document

slug: mirai-2-5-0
title: mirai 2.5.0
date: 2025-09-04
author: Charlie Gao
description: >
    mirai - minimalist async evaluation framework for R - brings production-grade
    parallel and distributed computing to the ecosystem.
photo:
  url: https://unsplash.com/photos/a-bunch-of-different-colored-sashes-hanging-on-a-wall-OEiN_lSyQqE
  author: Matt Benson

# one of: "deep-dive", "learn", "package", "programming", "roundup", or "other"
categories: [package] 
tags: [mirai, parallelism]
---

<!--
TODO:
* [x] Look over / edit the post's title in the yaml
* [x] Edit (or delete) the description; note this appears in the Twitter card
* [x] Pick category and tags (see existing with `hugodown::tidy_show_meta()`)
* [x] Find photo & update yaml metadata
* [x] Create `thumbnail-sq.jpg`; height and width should be equal
* [x] Create `thumbnail-wd.jpg`; width should be >5x height
* [x] `hugodown::use_tidy_thumbnails()`
* [x] Add intro sentence, e.g. the standard tagline for the package
* [x] `usethis::use_tidy_thanks()`
-->

We're excited to announce [mirai](https://mirai.r-lib.org) 2.5.0, bringing production-grade async computing to R!

This milestone release delivers enhanced observability through OpenTelemetry, reproducible parallel RNG, and key user interface improvements for compute profiles. We've also packed in twice as many [changes](https://mirai.r-lib.org/news/index.html) as usual - going all out in delivering a round of quality-of-life fixes and improvements to make your use of mirai even smoother!

You can install it from CRAN with:

```{r, eval = FALSE}
install.packages("mirai")
```

## Introduction to mirai

mirai (Japanese for 'future') provides a clean, modern approach to parallel computing in R.
Built on current communication technologies, it delivers extreme performance through professional-grade scheduling and an event-driven architecture.

It continues to evolve as the foundation for asynchronous and parallel computing across the R ecosystem, powering everything from [async Shiny](https://rstudio.github.io/promises/articles/promises_04_mirai.html) applications to [parallel map](https://www.tidyverse.org/blog/2025/07/purrr-1-1-0-parallel/) in purrr to [hyperparameter tuning](https://tune.tidymodels.org/news/index.html#parallel-processing-2-0-0) in tidymodels.

```{r}
library(mirai)

# Set up persistent background processes
daemons(4)

# Async evaluation - non-blocking
m <- mirai({
  Sys.sleep(1)
  100 + 42
})
m

# Results are available when ready
m[]

# Shut down persistent background processes
daemons(0)
```

## A unique design philosophy

### Modern foundation

mirai builds on [nanonext](https://nanonext.r-lib.org), the R binding to Nanomsg Next Generation, a high-performance messaging library designed for distributed systems. This means that it's using the very latest technologies, and supports the most optimal connections out of the box: IPC (inter-process communications), TCP or secure TLS. It also extends base R's serialization mechanism to support custom serialization of newer cross-language data formats such as safetensors, Arrow and Polars.

### Extreme performance

As a consequence of its solid technological foundation, mirai has the proven capacity to scale to millions of concurrent tasks over thousands of connections. Moreover, it delivers up to 1,000x the efficiency and responsiveness of common alternatives. A key innovation is the implementation of event-driven promises that react with zero latency - this provides an extra edge for real-time applications such as live inference or Shiny apps.

### Production first

mirai provides a clear mental model for parallel computation, with a clean separation of a user's current environment with that in which a mirai is evaluated. This explicitness and simplicity helps avoid common pitfalls that can afflict parallel processing, such as capturing incorrect or extraneous variables. Transparency and robustness are key to mirai's design, and are achieved by minimizing complexity, and eliminating all hidden state (no reliance on options or environment variables). Finally, its integration with OpenTelemetry provides for production-grade observability.

### Deploy everywhere

Deployment of daemons is made through a consistent interface across local, remote (SSH), and [HPC environments](https://shikokuchuo.net/posts/27-mirai-240/) (Slurm, SGE, PBS, LSF). Compute profiles are daemons settings that are managed independently, such that you can be connected to all three resource types simultaneously. You then have the freedom to distribute workload to the most appropriate resource for any given task - especially important for heterogeneous workloads that require different resources such as GPU compute.

## OpenTelemetry integration

New in mirai 2.5.0: complete observability of mirai requests through OpenTelemetry traces. This is a core feature that completes the final pillar in mirai's 'built for production' design philosophy.

When tracing is enabled via the otelsdk package, you can monitor the entire lifecycle of your async computations, from creation through to evaluation, making it easier to debug and optimize performance in production environments. This is especially powerful when used in conjunction with other otel-enabled packages (like an upcoming Shiny release), providing end-to-end observability across your entire application stack.

![*Illustrative span structure shown in a Jaeger collector UI*](otel-screenshot.png)

## Reproducible parallel RNG

Introduced in mirai 2.4.1: reproducible parallel random number generation. Developed in consultation with our tidymodels colleagues and core members of the mlr team, this is a great example of the R community pulling together to sovle common problems. It solves a long-standing challenge in parallel computing in R, and we hope it's put to good use for reproducible science.

mirai has long used L'Ecuyer-CMRG streams (since prior to v1.0) for statistically-sound parallel RNG. Streams essentially cut into the RNGâ€™s period (a very long sequence of pseudo-random numbers) at intervals that are far apart from each other that they do not in practice overlap. This ensures that statistical results obtained from parallel computations remain correct and valid. 

Previously, we only offered the following option, matching the behaviour of base R's parallel package:

**Default behaviour (`seed = NULL`)**: Creates independent streams for each daemon, ensuring statistical validity but not numerical reproducibility between runs.

Now, we offer the following option as well:

**Reproducible mode (`seed = integer`)**: Creates a stream for each `mirai()` call rather than each daemon, guaranteeing identical results across runs regardless of the number of daemons used.

```{r}
# Always provides identical results:

with(
  daemons(3, seed = 1234L),
  mirai_map(1:3, rnorm, .args = list(mean = 20, sd = 2))[]
)
```

## User interface improvements

### Compute profile helper functions

`with_daemons()` and `local_daemons()` make working with compute profiles much more convenient by allowing the temporary switching of contexts:

```r
# Work with specific compute profiles
with_daemons("gpu", {
  result <- mirai(gpu_intensive_task())
})

# Local version for use inside functions
async_gpu_intensive_task <- function() {
  local_daemons("gpu")
  mirai(gpu_intensive_task())
}
```

### Re-designed `daemons()`

Made to be more ergonomic, creating new daemons now automatically resets existing ones (no more manual `daemons(0)` calls):

```r
# Old approach
daemons(0)  # Had to reset first
daemons(4)

# New approach - automatic reset
daemons(4)  # Just works, resets if needed
```

### New `info()` function

Provides a more succinct alternative to `status()` for reporting key statistics.
This is optimized and supported for programmatic use:

```{r, eval = FALSE}
info()
#> connections  cumulative    awaiting   executing   completed 
#>           4           4           8           4           2
```

## Acknowledgements

We extend our gratitude to the R community for their continued feedback and contributions. Special thanks to all contributors who helped shape this release through feature requests, bug reports, and code contributions: [&#x0040;agilly](https://github.com/agilly), [&#x0040;D3SL](https://github.com/D3SL), [&#x0040;DavZim](https://github.com/DavZim), [&#x0040;dipterix](https://github.com/dipterix), [&#x0040;eliocamp](https://github.com/eliocamp), [&#x0040;erydit](https://github.com/erydit), [&#x0040;karangattu](https://github.com/karangattu), [&#x0040;louisaslett](https://github.com/louisaslett), [&#x0040;mikkmart](https://github.com/mikkmart), [&#x0040;sebffischer](https://github.com/sebffischer), [&#x0040;shikokuchuo](https://github.com/shikokuchuo), and [&#x0040;wlandau](https://github.com/wlandau).
