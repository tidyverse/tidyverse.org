---
output: hugodown::hugo_document

slug: tidyclust-0-1-0
title: tidyclust is on CRAN
date: 2022-12-06
author: Emil Hvitfeldt
description: >
    Tidyclust is on CRAN. tidyclust provides a common interface for specifying 
    clustering models, in the same style as parsnip.

photo:
  url: https://unsplash.com/photos/4Xy08NbMBLM
  author: Ankush Minda

# one of: "deep-dive", "learn", "package", "programming", "roundup", or "other"
categories: [package] 
tags: [tidymodels, tidyclust]
---

<!--
TODO:
* [X] Look over / edit the post's title in the yaml
* [X] Edit (or delete) the description; note this appears in the Twitter card
* [X] Pick category and tags (see existing with `hugodown::tidy_show_meta()`)
* [X] Find photo & update yaml metadata
* [X] Create `thumbnail-sq.jpg`; height and width should be equal
* [X] Create `thumbnail-wd.jpg`; width should be >5x height
* [X] `hugodown::use_tidy_thumbnails()`
* [X] Add intro sentence, e.g. the standard tagline for the package
* [X] `usethis::use_tidy_thanks()`
-->

We're very pleased to announce the release of [tidyclust](https://tidyclust.tidymodels.org/) 0.1.0. tidyclust is the tidymodels extension to fit and use clustering models.

You can install it from CRAN with:

```{r, eval = FALSE}
install.packages("{package}")
```

This blog post will introduce tidyclust, how to use it with the rest of tidymodels, and how we can interact and evaluate the fitted cluster models.

```{r setup}
library(tidymodels)
library(tidyclust)
```

## Specifying cluster models

The first thing we need to do is decide on the type of clustering model we want to fit. The pkgdown site provides a [list of all clustering specifications](https://tidyclust.tidymodels.org/reference/index.html#specifications) provided by tidyclust. We are slowly adding more types of models and [suggestions in issues](https://github.com/tidymodels/tidyclust/issues) are highly welcome!

We will use a K-Means model for these examples using `k_means()` to create a specification. tidyclust as with the rest of tidymodels tries to use informative names for functions and arguments, so we will be using `num_clusters` instead of `k`

```{r}
kmeans_spec <- k_means(num_clusters = 4) %>%
  set_engine("ClusterR")
kmeans_spec
```

We can use the `set_engine()`, `set_mode()`, and `set_args()` we are familiar with from parsnip. The specification itself isn't worth much if we don't apply it to some data. We will use the ames data set from the modeldata package

```{r}
data("ames", package = "modeldata")
```

this data set contains a number of categorical variables that unaltered can't be used with a k-means model. Some light preprocessing can be done using the recipes package.

```{r}
rec_spec <- recipe(~ ., data = ames) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_pca(all_numeric_predictors(), threshold = 0.8)
```

This recipe ensures that all the variables are numeric, normalized and have PCA applied to them to remove some of the correlation and number of features. Notice how we didn't specify an outcome as clustering models are unsupervised, meaning that we don't have outcomes.

These two specifications can be combined in a `workflow()` 

```{r}
kmeans_wf <- workflow(rec_spec, kmeans_spec)
```

And fit them together

```{r}
kmeans_fit <- fit(kmeans_wf, data = ames)
kmeans_fit
```

We have arbitrarily set the number of clusters to 4 above. But if we wanted to figure out what values would be "optimal" we would have to fit multiple models. We can do this with `tune_cluster()`, for this we need to use `tune()` to specify that `num_clusters` is the argument we want to try with multiple values

```{r}
kmeans_spec <- kmeans_spec %>% 
  set_args(num_clusters = tune())

kmeans_wf <- workflow(rec_spec, kmeans_spec)
kmeans_wf
```

Using bootstraps to fit multiple models for each value of `num_clusters` and we can use `tune_cluster` much the same way we use `tune_grid()`

```{r}
set.seed(1234)
boots <- bootstraps(ames, times = 10)

tune_res <- tune_cluster(
  kmeans_wf,
  resamples = boots
)
```

and the respective `collect_*()` functions work as with tune output

```{r}
collect_metrics(tune_res)
```

## Extraction

Going back to the first model with fit, in tidyclust there are 3 main things we can do with a fitted cluster model:

- extract cluster assignments
- extract centroid locations
- prediction with new data

Each of these tasks has a function associated with them. First, we have `extract_cluster_assignment()`, which can be used on fitted tidyclust objects, alone or as a part of a workflow and it returns the cluster assignment as a factor named `.cluster`in a tibble.

```{r}
extract_cluster_assignment(kmeans_fit)
```

The location of the clusters can be found using `extract_centroids()` which again returns a tibble, with `.cluster` being a factor with the same levels as earlier.

```{r}
extract_centroids(kmeans_fit)
```

Lastly, if the model has a notion that translates to "prediction", then `predict()` will give you those results as well. In the case of K-means, this is being interpreted as "which centroid is this observation closest to".

```{r}
predict(kmeans_fit, new_data = slice_sample(ames, n = 10))
```

Please check the [pkgdown site](https://tidyclust.tidymodels.org/) for more in-depth articles.

## Acknowledgements

A big thanks to all the contributors: [&#x0040;aephidayatuloh](https://github.com/aephidayatuloh), [&#x0040;avishaitsur](https://github.com/avishaitsur), [&#x0040;bryanosborne](https://github.com/bryanosborne), [&#x0040;cgoo4](https://github.com/cgoo4), [&#x0040;coforfe](https://github.com/coforfe), [&#x0040;EmilHvitfeldt](https://github.com/EmilHvitfeldt), [&#x0040;JauntyJJS](https://github.com/JauntyJJS), [&#x0040;kbodwin](https://github.com/kbodwin), [&#x0040;malcolmbarrett](https://github.com/malcolmbarrett), [&#x0040;mattwarkentin](https://github.com/mattwarkentin), [&#x0040;ninohardt](https://github.com/ninohardt), [&#x0040;nipnipj](https://github.com/nipnipj), and [&#x0040;tomazweiss](https://github.com/tomazweiss).
