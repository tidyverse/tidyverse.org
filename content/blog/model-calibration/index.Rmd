---
output: hugodown::hugo_document

slug: model-calibration
title: Model Calibration
date: 2022-11-17
author: Edgar Ruiz
description: >
    Model Calibration is coming to Tidymodels. This post covers the new plotting
    functions, and our plans for future enhancements. 

photo:
  url: https://unsplash.com/photos/n6vS3xlnsCc
  author: Bubbles the Puppy :) 

categories: [package]
tags: [model, plots]
---

<!--
TODO:
* [ ] Look over / edit the post's title in the yaml
* [ ] Edit (or delete) the description; note this appears in the Twitter card
* [ ] Pick category and tags (see existing with `hugodown::tidy_show_meta()`)
* [ ] Find photo & update yaml metadata
* [ ] Create `thumbnail-sq.jpg`; height and width should be equal
* [ ] Create `thumbnail-wd.jpg`; width should be >5x height
* [ ] `hugodown::use_tidy_thumbnails()`
* [ ] Add intro sentence, e.g. the standard tagline for the package
* [ ] `usethis::use_tidy_thanks()`
-->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  #eval = FALSE
)

library(probably)
library(dplyr)
library(ggplot2)
library(tidymodels)
```

I am very excited to introduce work currently underway.  We are looking to create 
early awareness, and to receive feedback from the community. That is why the 
enhancements discussed here are not yet in CRAN. As probably inferred by the title 
and description, the work centers around bringing Model Calibration to Tidymodels.

Even though the article is meant to introduce new package functionality. We also
have the goal of introducing Model Calibration conceptually. The idea is to 
encourage everyone who may have use for it to try it out, even though this may
be the first time that you read about Model Calibration.  So, if you are already
familiar with Model Calibration, feel free to skip to the [Setup](#setup) section
to get started. 

## Model Calibration

*The goal of Model Calibration is to improve the accuracy of predictions.* It 
does this by adjusting the prediction's probabilities. Meaning that the predicted 
outcome may change based on the established threshold. 

For example, a prediction could say that it is 60% certain of a result of "Yes".
But, after applying the calibration, the new probability is now 45%. If the 
threshold is set to 50%, the new predicted outcome is now set to "No". 

There are two main components to Model Calibration:

- **Diagnosis** - Figuring out how well the original, and calibrated probabilities 
perform
- **Remediation** - Calculating, and applying the calibration


## The plan

As with everything in machine learning, there are several options to
consider when calibrating a model. Through the new features in the Tidymodels
packages, we aspire to make those options as easily accessible as possible. 

Our plan is to implement Model Calibration in two phases: The first phase will focus
on binary models, and the second phase will focus on multi-class models. 

The first batch of enhancements are now available in the development version of 
`probably`. The enhancements are centered around plotting functions meant for
**diagnosing** the prediction's performance. These are more commonly known as 
**Calibration Plots**.

## Calibration plots

The idea behind a Calibration Plot is that if we group the predictions based on 
their probability, then we should see an percentage of events ^[We can think of 
an **event** as the outcome that is being tracked by the probability. For 
example, in a model predicting "heads" or "tails", and we want to calibrate the
probability for "tails", then the **event** is when the column containing the 
outcome, has the value of "tails".]  that match such probability. 

For example, if we group all of the predictions with 10% probability. We should
expect that about 10% of the those in the group are indeed events. We should 
also expect about 20% of those with 20% probability to be events, and so forth. 

The Calibration Plot for the ideal model will essentially be perfect incline line
that start at 0,0 and ends in 1,1. But, as with everything in real life, there
is really no model that can accomplish this. Sometimes the model will predict
a 10% probability, and no event is found. Which means that those probabilities
should be adjusted down to 0.

Here is an example of the new Calibration Plots in `probably`. We will discuss
the particulars of the functions in the next section. For now, we are simply 
introducing what the plot would look like:

```{r, echo = FALSE}
segment_logistic %>% 
  cal_plot_breaks(
    Class, 
    .pred_good, 
    include_rug = FALSE,
    include_points = FALSE,
    include_ribbon = FALSE
    )
```

In the case of this model, in can see that the group in the about 40% probability
(Bin Midpoint) actually has about 60%  event rate. If we were to calibrate
our model, and increase the probability for those, then these could change the
predicted outcome, assuming that the threshold is set for 50%. On the other hand,
those with 55% - 60% probability, seem have an event rate under 50%. Those could
also be calibrated to change the prediction, if again, the threshold is set to 50%.

That is the power of calibrating probabilities. The trick is knowing where to
calibrate, and which direction to adjust (up or down).

## Setup

To try out the new calibration plots, install the development version of `probably`:

```{r, eval = FALSE}
remotes::install_github("tidymodels/probably")
```

If you would like to follow along, load the `probably` and `dplyr` packages 
into you R session.

```{r}
library(probably)
library(dplyr)
```

`probably` comes with a few data sets. For most of the examples in this post,
we will use `segment_logistic`. It is an example data set that contains
predictions, and their probabilities. `Class` contains the outcome of 
"good" and "poor", `.pred_good` contains the probability that the event is "good". 

```{r}
segment_logistic
```


## Binned plot

On smaller data sets, it is a challenging to obtain an accurate *event rate* 
for a given probability. For example, if there are 5 predictions with a 50% 
probability, and 3 of those are events, the plot would show a 60% event rate.
This comparison would not be appropriate because there are no enough predictions
to really determine how close to 50% the model really is. 

The most common approach to plotting the performance, is to group the probabilities
into bins, or buckets.  Usually, the data is split into 10 discrete buckets,
from 0 to 1 (0 - 100%). The *event rate* and the *median of the probabilities* is 
calculated for each bin. 

In `probably`, binned calibration plots can be created using `cal_plot_breaks()`. 
It expects a data set, and the un-quoted variable names that contains the events
(`truth`), and the probabilities (`estimate`). For the example here, we pass the 
`segment_logistic` data set, and use `Class` and `.pred_good` as the arguments. 
By default, this function will create a calibration plot with 10 buckets (breaks):

```{r}
segment_logistic %>% 
  cal_plot_breaks(Class, .pred_good)
```

The number of bins in `cal_plot_breaks()` can be adjusted using `num_breaks`. 
Here is an example of what the plot looks like if we reduce the bins from 10,
to 5:

```{r}
segment_logistic %>% 
  cal_plot_breaks(Class, .pred_good, num_breaks = 5 )
```

## Windowed

Another approach is to use overlapping ranges, or windows. We can calculate the
midpoint and the event rate for each of the windows. And because we are plotting 
the median for each window, even though the ranges overlap, the midpoints will
not.  This method reduces the risk  of any discrete bin over or under estimating 
the event rate.

There are two variables that control the windows. The **step size**, controls
the frequency of the windows. If we set a step size of 5%, will create a new
window every 5% probability (5%, 10%, 15%... etc). The second argument is
**window size**. This defines how big the the window is.  If it is set to %10, 
then a given step will overlap halfway into the previous step, as well as the
next step. Here is a visual representation of this specific scenario:

```{r, echo = FALSE}
windows_size <- 0.10
step_size    <- 0.05

steps <- seq(0, 1, by = step_size)
lower_cut <- steps - (windows_size / 2)
lower_cut[lower_cut < 0] <- 0
upper_cut <- steps + (windows_size / 2)
upper_cut[upper_cut > 1] <- 1

tibble(
  step = steps,
  lower = lower_cut,
  upper = upper_cut,
  group = seq_along(steps)
) %>% 
  ggplot(aes(x = step, y = group)) + 
  geom_errorbar(aes(xmin = lower, xmax = upper)) +
  theme_minimal() +
  theme(aspect.ratio = 1)
```

In `probably`, the `cal_plot_windowed()` function provides this functionality. 
The default step size is 0.05, and can be changed via the `step_size` argument.
The default window size is 0.1, and can be changed via the `window_size` argument.

```{r}
segment_logistic %>% 
  cal_plot_windowed(Class, .pred_good)
```

Here is an example of reducing the `step_size` from 0.05, to 0.02. There are
more than double the windows:

```{r}
segment_logistic %>% 
  cal_plot_windowed(Class, .pred_good, step_size = 0.02)
```

## Logistic

Another way to visualize the performance is to fit a regression model of the
events against the probabilities. This is helpful because it avoids the use of
pre-determined groupings. Another difference, is that we are not plotting 
midpoints of actual results, but rather predictions based on those results. 

The `cal_plot_logistic()` provides this functionality. By default, it uses a 
logistic spline model, provided by the `mgcv` package. The idea is to visualize
a smooth line based on the predictions:

```{r}
segment_logistic %>% 
  cal_plot_logistic(Class, .pred_good)
```

If smoothing is not needed, a more straightforward logistic model can be used
to fit the probabilities. By setting the `smooth` argument to `FALSE`, will make
the function switch to the results from a  `glm()` model as the base of the
visualization. 

```{r}
segment_logistic %>% 
  cal_plot_logistic(Class, .pred_good, smooth = FALSE)
```

## Additional options

```{r}
segment_logistic %>% 
  cal_plot_breaks(Class, .pred_good, conf_level = 0.8)
```
```{r}
segment_logistic %>% 
  cal_plot_windowed(Class, .pred_good, include_points = FALSE)
```

## Integration with `tune`

```{r, eval = FALSE}
library(tidymodels)

set.seed(111)

sim_data <- sim_classification(500)

rec <- recipe(class ~ ., data = sim_data) %>%
  step_ns(linear_01, deg_free = tune("linear_01"))

tuned_model <- tune_grid(
  object = set_engine(logistic_reg(), "glm"),
  preprocessor = rec,
  resamples = vfold_cv(sim_data, v = 2, repeats = 3),
  control = control_resamples(save_pred = TRUE)
)
```

```{r, eval = FALSE}
tuned_model %>% 
  cal_plot_breaks()
```

## Getting ready for the next stage


```{r}
model <- glm(Class ~ .pred_good, segment_logistic, family = "binomial")

preds <- predict(model, segment_logistic, type = "response")
  
combined <- bind_rows(
  mutate(segment_logistic, source = "original"), 
  mutate(segment_logistic, .pred_good = 1 - preds, source = "glm")
  )

combined 
```


```{r}
combined %>%
  group_by(source) %>%
  cal_plot_breaks(Class, .pred_good)
```

```{r}
combined %>% 
  group_by(source) %>% 
  cal_plot_breaks(Class, .pred_good)
```

