---
output: hugodown::hugo_document

slug: model-calibration
title: Model Calibration
date: 2022-11-17
author: Edgar Ruiz
description: >
    Model Calibration is coming to tidymodels. This post covers the new plotting
    functions, and our plans for future enhancements. 

photo:
  url: https://unsplash.com/photos/iLKK0eFTywU
  author: Graphic Node

categories: [package]
tags: [model, plots]
---

<!--
TODO:
* [ ] Look over / edit the post's title in the yaml
* [ ] Edit (or delete) the description; note this appears in the Twitter card
* [ ] Pick category and tags (see existing with `hugodown::tidy_show_meta()`)
* [ ] Find photo & update yaml metadata
* [ ] Create `thumbnail-sq.jpg`; height and width should be equal
* [ ] Create `thumbnail-wd.jpg`; width should be >5x height
* [ ] `hugodown::use_tidy_thumbnails()`
* [ ] Add intro sentence, e.g. the standard tagline for the package
* [ ] `usethis::use_tidy_thanks()`
-->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  #eval = FALSE
)

library(probably)
library(dplyr)
library(ggplot2)
library(tidymodels)
```

I am very excited to introduce work currently underway.  We are looking to create 
early awareness, and to receive feedback from the community. That is why the 
enhancements discussed here are not yet in CRAN. 

Even though the article is meant to introduce new package functionality. We also
have the goal of introducing model calibration conceptually. We want to provide
sufficient background for those who may not be familiar with model calibration.
If you are already familiar with this technique, feel free to skip to the 
[Setup](#example-data) section to get started. 

To install the version of probably used here: 

```{r, eval = FALSE}
remotes::install_github("tidymodels/probably")
```


## Model Calibration

*The goal of model calibration is to ensure that the estimated class probabilities 
consistent with what would naturally occur.* If a model has poor calibration, 
we might be able to post-process the original predictions to coerce them to have
better properties. 

There are two main components to model calibration:

- **Diagnosis** - Figuring out how well the original (and re-calibrated) probabilities 
perform.
- **Remediation** - Adjusting the original values to have better properties.


## The development plan

As with everything in machine learning, there are several options to
consider when calibrating a model. Through the new features in the tidymodels
packages, we aspire to make those options as easily accessible as possible. 

Our plan is to implement model calibration in two phases: The first phase will focus
on binary models, and the second phase will focus on multi-class models. 

The first batch of enhancements are now available in the development version of 
probably. The enhancements are centered around plotting functions meant for
**diagnosing** the prediction's performance. These are more commonly known as 
**calibration plots**.

## Calibration plots

The idea behind a calibration plot is that if we group the predictions based on 
their probability, then we should see an percentage of events ^[We can think of 
an **event** as the outcome that is being tracked by the probability. For 
example, in a model predicting "heads" or "tails", and we want to calibrate the
probability for "tails", then the **event** is when the column containing the 
outcome, has the value of "tails".]  that match such probability. 

For example, if we collect a group of the predictions whose probabilities are 
estimated to be about 10%. We should expect that about 10% of the those in the 
group to indeed be events. 

The calibration plot for the ideal model will essentially be perfect incline line
that start at (0,0) and ends in (1,1). But, as with everything in real life, there
is really no model that can accomplish this. Sometimes the model will predict
a 10% probability, and no event is found. Which means that those probabilities
should be adjusted to have smaller probabilities. 

Let's look at an example of the new calibration plots in probably. We will discuss
the particulars of the functions in the next section. For now, we are simply 
introducing what the plot would look like:

```{r, echo = FALSE}
segment_logistic %>% 
  cal_plot_breaks(
    Class, 
    .pred_good, 
    include_rug = FALSE,
    include_points = FALSE,
    include_ribbon = FALSE
    )
```

In the case of this model, in can see that the group in the about 40% probability
(Bin Midpoint) actually has about 60%  event rate. If we were to calibrate
our model, and increase the probability for those, then these could change the
predicted outcome, assuming that the threshold is set for 50%. On the other hand,
those with 55% - 60% probability, seem have an event rate under 50%. Those could
also be calibrated to change the prediction, if again, the threshold is set to 50%.

## Example data

If you would like to follow along, load the probably and dplyr packages 
into you R session.

```{r}
library(probably)
library(dplyr)
```

probably comes with a few data sets. For most of the examples in this post,
we will use `segment_logistic`. It is an example data set that contains
predictions, and their probabilities. `Class` contains the outcome of 
"good" and "poor", `.pred_good` contains the probability that the event is "good". 

```{r}
segment_logistic
```


## Binned plot

On smaller data sets, it is a challenging to obtain an accurate *event rate* 
for a given probability. For example, if there are 5 predictions with a 50% 
probability, and 3 of those are events, the plot would show a 60% event rate.
This comparison would not be appropriate because there are no enough predictions
to really determine how close to 50% the model really is. 

The most common approach to plotting the performance, is to group the probabilities
into bins, or buckets.  Usually, the data is split into 10 discrete buckets,
from 0 to 1 (0 - 100%). The *event rate* and the *median of the probabilities* is 
calculated for each bin. 

In probably, binned calibration plots can be created using `cal_plot_breaks()`. 
It expects a data set, and the un-quoted variable names that contains the events
(`truth`), and the probabilities (`estimate`). For the example here, we pass the 
`segment_logistic` data set, and use `Class` and `.pred_good` as the arguments. 
By default, this function will create a calibration plot with 10 buckets (breaks):

```{r}
segment_logistic %>% 
  cal_plot_breaks(Class, .pred_good)
```

The number of bins in `cal_plot_breaks()` can be adjusted using `num_breaks`. 
Here is an example of what the plot looks like if we reduce the bins from 10,
to 5:

```{r}
segment_logistic %>% 
  cal_plot_breaks(Class, .pred_good, num_breaks = 5 )
```

## Windowed

Another approach is to use overlapping ranges, or windows. We can calculate the
midpoint and the event rate for each of the windows. And because we are plotting 
the median for each window, even though the ranges overlap, the midpoints will
not.  This method reduces the risk  of any discrete bin over or under estimating 
the event rate.

There are two variables that control the windows. The **step size**, controls
the frequency of the windows. If we set a step size of 5%, will create a new
window every 5% probability (5%, 10%, 15%... etc). The second argument is
**window size**. This defines how big the the window is.  If it is set to %10, 
then a given step will overlap halfway into the previous step, as well as the
next step. Here is a visual representation of this specific scenario:

```{r, echo = FALSE}
windows_size <- 0.10
step_size    <- 0.05

steps <- seq(0, 1, by = step_size)
lower_cut <- steps - (windows_size / 2)
lower_cut[lower_cut < 0] <- 0
upper_cut <- steps + (windows_size / 2)
upper_cut[upper_cut > 1] <- 1

tibble(
  step = steps,
  lower = lower_cut,
  upper = upper_cut,
  group = seq_along(steps)
) %>% 
  ggplot(aes(x = step, y = group)) + 
  geom_errorbar(aes(xmin = lower, xmax = upper)) +
  theme_minimal() +
  theme(aspect.ratio = 1)
```

In probably, the `cal_plot_windowed()` function provides this functionality. 
The default step size is 0.05, and can be changed via the `step_size` argument.
The default window size is 0.1, and can be changed via the `window_size` argument.

```{r}
segment_logistic %>% 
  cal_plot_windowed(Class, .pred_good)
```

Here is an example of reducing the `step_size` from 0.05, to 0.02. There are
more than double the windows:

```{r}
segment_logistic %>% 
  cal_plot_windowed(Class, .pred_good, step_size = 0.02)
```

## Logistic

Another way to visualize the performance is to fit a regression model of the
events against the probabilities. This is helpful because it avoids the use of
pre-determined groupings. Another difference, is that we are not plotting 
midpoints of actual results, but rather predictions based on those results. 

The `cal_plot_logistic()` provides this functionality. By default, it uses a 
logistic spline model, provided by the `mgcv` package. The idea is to visualize
a smooth line based on the predictions:

```{r}
segment_logistic %>% 
  cal_plot_logistic(Class, .pred_good)
```

If smoothing is not needed, a more straightforward logistic model can be used
to fit the probabilities. By setting the `smooth` argument to `FALSE`, will make
the function switch to the results from a  `glm()` model as the base of the
visualization. 

```{r}
segment_logistic %>% 
  cal_plot_logistic(Class, .pred_good, smooth = FALSE)
```

## Additional options and features

### Intervals

The confidence intervals are visualized using the gray ribbon. The default
interval is 0.9, but can be changed using the `conf_level` argument.  

```{r}
segment_logistic %>% 
  cal_plot_breaks(Class, .pred_good, conf_level = 0.8)
```

If desired, the intervals can be removed by setting the `include_ribbon` 
argument to `FALSE`.

```{r}
segment_logistic %>% 
  cal_plot_breaks(Class, .pred_good, include_ribbon = FALSE)
```

### Rugs

By default, the calibration plots include a RUGs layer at the top and at the 
bottom of the visualization.  They are meant to give us an idea of the density
of events, versus the density of non-events as the probabilities progress from
0 to 1. 

```{r, echo = FALSE}
segment_logistic %>% 
  cal_plot_breaks(Class, .pred_good) +
  geom_segment(
    x = 0.5, 
    xend = 0.6, 
    y = 0.9, 
    yend = 1, 
    arrow = arrow(length = unit(0.3, "cm")),
    color = "#ffaaff"
    ) +
  geom_text(x = 0.5, y = 0.87, label = "Density of events", color = "#ff88ff")  +
  geom_segment(
    x = 0.5, 
    xend = 0.6, 
    y = 0.1, 
    yend = 0.0, 
    arrow = arrow(length = unit(0.3, "cm")),
    color = "#ffaaff"
    ) +
  geom_text(x = 0.5, y = 0.13, label = "Density of non-events", color = "#ff88ff")
```

This can layer can be removed by setting `include_rug` to `FALSE`:

```{r}
segment_logistic %>% 
  cal_plot_breaks(Class, .pred_good, include_rug = FALSE) 
```


## Integration with the tune package

The calibration plots in probably support `tune_results` objects.  The functions
read the metadata from the tune object, and the `truth` and `estimate` 
arguments automatically. 

To showcase this feature, we will tune a model based on simulated data. In order
for the calibration plot to work, the predictions need to be collected. This is
done by setting `save_pred` to `TRUE`.

```{r, eval = TRUE}
library(tidymodels)

set.seed(111)

sim_data <- sim_classification(500)

rec <- recipe(class ~ ., data = sim_data) %>%
  step_ns(linear_01, deg_free = tune("linear_01"))

tuned_model <- tune_grid(
  object = set_engine(logistic_reg(), "glm"),
  preprocessor = rec,
  resamples = vfold_cv(sim_data, v = 2, repeats = 3),
  
  # Important: `saved_pred` has to be set to TRUE in order for 
  #the plotting to be possible
  
  control = control_resamples(save_pred = TRUE)
)

tuned_model
```
The plotting functions will automatically collect the predictions. Each of the 
pre-processing groups will be plotted individually in its own facet.

```{r, eval = TRUE, fig.dim = c(9, 9)}
tuned_model %>% 
  cal_plot_logistic() 
```

## Preparing for the next stage

As mentioned in the outset of this post, the goal is to also provide a way to
calibrate the model, and to apply the calibration to future predictions.  We have
made sure that the plotting functions are ready now to accept multiple probability
sets.

In this post, we will showcase that functionality by "manually" creating a quick 
calibration model, we we can use it to compare to the original probabilities. We will
need both of them to be on the same data frame, and to have a way of
distinguishing the original probabilities from the calibrated probabilities. In
this case we will create a variable called `source`:

```{r}
model <- glm(Class ~ .pred_good, segment_logistic, family = "binomial")

preds <- predict(model, segment_logistic, type = "response")
  
combined <- bind_rows(
  mutate(segment_logistic, source = "original"), 
  mutate(segment_logistic, .pred_good = 1 - preds, source = "glm")
  )

combined 
```

The new plot functions support dplyr groupings. So, to overlay the two
groups, we just need to pass `source` to `group_by()`: 

```{r}
combined %>%
  group_by(source) %>%
  cal_plot_breaks(Class, .pred_good)
```
If we would like to plot them side by side, we can add `facet_wrap()` as an
additional step of the plot:

```{r}
library(ggplot2)

combined %>% 
  group_by(source) %>% 
  cal_plot_breaks(Class, .pred_good) +
  facet_wrap(~source) +
  theme(legend.position = "none")
```
Our goal in the future is to provide calibration functions that create the 
models, and provide an easy way to visualize.


## Conclusion

As mentioned at the top of this post. We look forward to your feedback as you try
out these features, and read about our plans for the new future. If you wish
to send us your thoughts, feel free to open an issue in probably's GitHub repo
here: https://github.com/tidymodels/probably/issues.

