---
output: hugodown::hugo_document
slug: tidymodels-sparse-support
title: Sparse data structures in tidymodels
date: 2020-11-16
author: Julia Silge
description: >
    Sparse data is common in many domains, and now tidymodels supports using 
    sparse matrix structures throughout the fitting and tuning stages of modeling.
photo:
  url: https://unsplash.com/photos/7JX0-bfiuxQ
  author: JJ Ying
# one of: "deep-dive", "learn", "package", "programming", or "other"
categories: [learn] 
tags: [tidymodels,tune,parsnip,hardhat]
---


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, cache.lazy = FALSE, warning = FALSE, 
                      message = FALSE, echo = TRUE, dpi = 180,
                      fig.width = 8, fig.height = 5)
library(tidyverse)
theme_set(theme_minimal())
```

The new release of [tune](https://www.tidyverse.org/blog/2020/11/tune-0-1-2/) is chock full of improvements and new features. This blog post is the second of three posts exploring the updates available in tune 0.1.2. When combined with the latest releases of [hardhat](http://hardhat.tidymodels.org/) and [parsnip](https://parsnip.tidymodels.org/), one upgrade that tidymodels users can now use in their day-to-day modeling work is some **support for sparse data structures** during fitting and tuning.

## Why sparse data?

In some subject matter domains, it is common to have lots and lots of zeroes after transforming data to a representation appropriate for analysis or modeling. Text data is one such example. The `small_fine_foods` dataset of Amazon reviews of fine foods contains a column `review` that we as humans can read and understand.

```{r}
library(tidyverse)
library(tidymodels)

data("small_fine_foods")
training_data
```

Computers, on the other hand, need that `review` variable to be heavily preprocessed and transformed in order for it to be ready for most modeling. We typically need to [tokenize](https://smltar.com/tokenization.html) the text, find word frequencies, and perhaps [compute tf-idf](https://www.tidytextmining.com/tfidf.html). There are quite a number of different structures we can use to store the results of this preprocessing. We can keep the results in a long, tidy tibble, which is excellent for exploratory data analysis.

```{r}
library(tidytext)

tidy_reviews <- training_data %>%
  unnest_tokens(word, review) %>%
  count(product, word) %>%
  bind_tf_idf(word, product, n)

tidy_reviews
```

We can also transform these results to a wide format, often a good fit when the next step is a modeling or machine learning algorithm.

```{r R.options = list(tibble.print_min=15, tibble.max_extra_cols=20)}
wide_reviews <- tidy_reviews %>%
  select(product, word, tf_idf) %>%
  pivot_wider(names_from = word, names_prefix = "word_",
              values_from = tf_idf, values_fill = 0)

wide_reviews
```

Lots of zeroes! Instead of using a tibble, we can transform these results to a **sparse matrix**, a specialized data structure that keeps track of only the non-zero elements instead of every element.

```{r, R.options=list(quanteda_print_dfm_max_ndoc = 0, quanteda_print_dfm_max_nfeat = 0)}
sparse_reviews <- tidy_reviews %>%
  cast_dfm(product, word, tf_idf)

sparse_reviews
```

As is typical for text data, this document-feature matrix is extremely sparse, with many zeroes. Most documents do not contain most words. By using this kind of specialized structure instead of anything like a vanilla `matrix` or `data.frame`, we secure two benefits:

- We can taken advantage of the **speed** gained from any specialized model algorithms built for sparse data.
- The amount of **memory** this object requires decreases dramatically.

How big of a change in memory are we talking about?

```{r}
lobstr::obj_sizes(wide_reviews, sparse_reviews)
```


## A blueprint for sparse models

Before the most recent releases of hardhat, parsnip, and tune, there was no support for sparse data structures within tidymodels. Now, you can specify a hardhat **blueprint** for sparse data.

```{r}
library(hardhat)
sparse_bp <- default_recipe_blueprint(composition = "dgCMatrix")
```

The `dgCMatrix` composition is from the [Matrix](https://cran.r-project.org/package=Matrix) package, and is the most standard class for sparse numeric matrices in modeling in R. (You can also specify a dense matrix composition with `composition = "matrix"`.)

## Workflows and sparsity

The blueprint is used under the hood by the hardhat functions to process data. To get ready to fit our model using the sparse blueprint, we can set up our preprocessing recipe:

```{r}
library(textrecipes)

text_rec <-
  recipe(score ~ review, data = training_data) %>%
  step_tokenize(review)  %>%
  step_stopwords(review) %>%
  step_tokenfilter(review, max_tokens = 1e3) %>%
  step_tfidf(review)
```

And we set up our model as we would normally:

```{r}
lasso_spec <-
  logistic_reg(penalty = 0.02, mixture = 1) %>%
  set_engine("glmnet")
```

The regularized modeling of the glmnet package is an example of an algorithm that has specialized approaches for sparse data. If we pass in dense data with `set_engine("glmnet")`, the underlying model will take one approach, but it will use a different, faster approach especially built for sparse data if we pass in a sparse matrix. Typically, we would recommend centering and scaling predictors using `step_normalize()` before fitting a regularized model like glmnet. However, if we do this, we would no longer have all our zeroes and sparse data. Instead, we can "normalize" these text predictors using tf-idf so that they are all on the same scale.

Let's put together two workflows, one using the sparse blueprint and one using the default behavior.

```{r}
wf_sparse <- 
  workflow() %>%
  add_recipe(text_rec, blueprint = sparse_bp) %>%
  add_model(lasso_spec)
  
wf_default <- 
  workflow() %>%
  add_recipe(text_rec) %>%
  add_model(lasso_spec)
```

## Comparing model results

Now let's use `fit_resamples()` to estimate how well this model fits with both options and measure performance for both.

```{r}
set.seed(123)
food_folds <- vfold_cv(training_data, v = 3)

results <- bench::mark(
  iterations = 10, check = FALSE,
  sparse = fit_resamples(wf_sparse, food_folds),  
  default = fit_resamples(wf_default, food_folds), 
)

results
```

We see on the order of a 10x speed gain by using the sparse blueprint!

```{r}
autoplot(results, type = "ridge")
```

The model performance metrics are the same:

```{r}
fit_resamples(wf_sparse, food_folds) %>%
  collect_metrics()

fit_resamples(wf_default, food_folds) %>%
  collect_metrics()
```

To see a detailed text modeling example using this dataset of food reviews, _without_ sparse encodings but complete with tuning hyperparameters, check out [our article on `tidymodels.org`](https://www.tidymodels.org/learn/work/tune-text/).

## Current limits

In tidymodels, the support for sparse data structures begins coming _out_ of a [preprocessing recipe](https://www.tmwr.org/recipes.html) and continues throughout the fitting and tuning process. We typically still expect the input _into_ a recipe to be a data frame, as shown in this text analysis example, and there is very limited support within tidymodels for starting with a sparse matrix, for example by using `parsnip::fit_xy()`.

There are currently three models in parsnip that support a sparse data encoding: 

- the glmnet engine for linear and logistic regression (including multinomial regression), 
- the XGBoost engine for boosted trees, and 
- the ranger engine for random forests.

There is heterogeneity in how recipes themselves handle data internally; this is why we didn't see a huge decrease in memory use when comparing `wf_sparse` to `wf_default`. The [textrecipes](https://textrecipes.tidymodels.org/) package internally adopts the idea of a [tokenlist](https://textrecipes.tidymodels.org/reference/tokenlist.html), which is memory efficient for sparse data, but other recipe steps may handle data in a dense tibble structure. Keep these current limits in mind as you consider the memory requirements of your modeling projects!

