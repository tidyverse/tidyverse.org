<!doctype html><html><head><!doctype html><html lang=en-us><link rel=stylesheet href=/css/fonts.css><link rel=stylesheet href=/css/main-site.css><link rel=stylesheet href=/css/fa5-all.css><style type=text/css>body{background-color:#fff;color:#1a1917}a{color:#38577f}a:hover{color:#42709b}a:focus{outline-color:#42709b}.column25-left .sectionTitle a:hover:not(.current),.column16-left .sectionTitle a:hover:not(.current){color:#1a1917}.icon-attribution,.icon-attribution a{color:#1a1917;opacity:75%}#homeContent .band.first{background-color:#fff}#homeContent .band.second{background-color:#fdeba4}#homeContent .band.third{background-color:#fff}#rStudioHeader{background-color:#1a162d;color:#fff}#rStudioHeader .productName{color:#fff}#rStudioHeader .productName:hover,#rStudioHeader .productName:focus{color:#fdeba4}#rStudioHeader #menu .menuItem.a{background-color:#75aadb;color:#fff}#rStudioHeader #menu .menuItem:hover{background-color:#484557;color:#fff}#rStudioHeader #menu .menuItem.current{background-color:#fff;color:#1a162d;text-decoration:none}#rStudioFooter.band{background-color:#767381}#rStudioFooter .bandContent #copyright{color:#e3eef8}table tbody tr:nth-child(even){background-color:#f7faff}table tbody tr:nth-child(odd){background-color:#fff}.latest{border-top:.5em solid <no value>}.event{-moz-box-shadow:0 0 0 0 rgba(0,0,0,.1);-webkit-box-shadow:0 0 0 0 rgba(0,0,0,.1);box-shadow:0 0 0 0 rgba(0,0,0,.1)}.section .event{background-color:#eab0c41a}.section .event{border-top:7pt solid #a19ea936}.section .event a{color:#1a162d}.section .event{color:#404040}</style><link rel=stylesheet href=/css/tidyverse.css><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo 0.71.1"><title>Sparse data structures in tidymodels</title><meta property="og:title" content="Sparse data structures in tidymodels"><meta property="og:description" content="Sparse data is common in many domains, and now tidymodels supports using  sparse matrix structures throughout the fitting and tuning stages of modeling."><meta property="og:type" content="article"><meta property="og:url" content="https://www.tidyverse.org/blog/2020/11/tidymodels-sparse-support/"><meta property="twitter:card" content="summary_large_image"><meta property="og:title" content="Sparse data structures in tidymodels - Tidyverse"><meta property="description" content="Sparse data is common in many domains, and now tidymodels supports using  sparse matrix structures throughout the fitting and tuning stages of modeling."><meta property="og:description" content="Sparse data is common in many domains, and now tidymodels supports using  sparse matrix structures throughout the fitting and tuning stages of modeling."><meta property="og:image" content="https://www.tidyverse.org/blog/2020/11/tidymodels-sparse-support/thumbnail-sq.jpg"><meta name=twitter:image content="https://www.tidyverse.org/blog/2020/11/tidymodels-sparse-support/thumbnail-wd.jpg"><link rel=apple-touch-icon sizes=180x180 href=/images/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/images/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/images/favicons/favicon-16x16.png><link rel=manifest href=/images/favicons/site.webmanifest><link rel=mask-icon href=/images/favicons/safari-pinned-tab.svg><link rel="shortcut icon" href=/images/favicons/favicon.ico><meta name=msapplication-TileColor content="#da532c"><meta name=msapplication-config href=/images/favicons/browserconfig.xml><script type=text/javascript src=/js/jquery-3.5.1.min.js></script><script type=text/javascript src=/js/site.js></script><link rel=icon href=/images/favicon.ico><script type=text/javascript src=/js/clipboard.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin=anonymous></script><script defer data-domain=tidyverse.org,all.tidyverse.org src=https://plausible.io/js/plausible.js></script></head><body><div id=appTidyverseSite class="shrinkHeader alwaysShrinkHeader"><div id=main><div id=rStudioHeader><nav class=band><div class="innards bandContent"><div><a class=productName href=/%20/>Tidyverse</a></div><div id=menu><div id=menuToggler></div><div id=menuItems><a class=menuItem href=/packages/>Packages</a>
<a class="menuItem current" href=/blog/>Blog</a>
<a class=menuItem href=/learn/>Learn</a>
<a class=menuItem href=/help/>Help</a>
<a class=menuItem href=/contribute/>Contribute</a></div></div></div></nav></div><main><div class="band padForHeader pushFooter"><div class=bandContent><div class="full splitColumns withMobileMargins"><div class=column75><h1 class=article-title>Sparse data structures in tidymodels</h1><div class=article-header aria-hidden=true><div class=photo><img src=/blog/2020/11/tidymodels-sparse-support/thumbnail-wd.jpg></div><div class=photoCredit>Photo by <a href=https://unsplash.com/photos/7JX0-bfiuxQ>JJ Ying</a></div></div><span class=article-date><i class="fas fa-calendar-day fa-fw"></i>&nbsp;&nbsp;2020/11/25</span><p style=margin-bottom:.5em;margin-top:.85em><i class="fas fa-tags"></i>&nbsp;
<a href=/tags/tidymodels/>tidymodels</a>,
<a href=/tags/parsnip/>parsnip</a>,
<a href=/tags/tune/>tune</a>,
<a href=/tags/hardhat/>hardhat</a></p><p style=margin-top:.5em><i class="fas fa-user-circle fa-fw"></i>&nbsp;
Julia Silge</p><div class=article-content><p>The new release of
<a href=https://www.tidyverse.org/blog/2020/11/tune-0-1-2/ target=_blank rel=noopener>tune</a> is chock full of improvements and new features. This blog post is the second of three posts exploring the updates available in tune 0.1.2. When combined with the latest releases of
<a href=http://hardhat.tidymodels.org/ target=_blank rel=noopener>hardhat</a> and
<a href=https://parsnip.tidymodels.org/ target=_blank rel=noopener>parsnip</a>, one upgrade that tidymodels users can now use in their day-to-day modeling work is some <strong>support for sparse data structures</strong> during fitting and tuning.</p><h2 id=why-sparse-data>Why sparse data?
<a href=#why-sparse-data><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>In some subject matter domains, it is common to have lots and lots of zeroes after transforming data to a representation appropriate for analysis or modeling. Text data is one such example. The <code>small_fine_foods</code> dataset of Amazon reviews of fine foods contains a column <code>review</code> that we as humans can read and understand.</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=nf>library</span><span class=p>(</span><span class=n>tidyverse</span><span class=p>)</span>
<span class=nf>library</span><span class=p>(</span><span class=n>tidymodels</span><span class=p>)</span>

<span class=nf>data</span><span class=p>(</span><span class=s>&#34;small_fine_foods&#34;</span><span class=p>)</span>
<span class=n>training_data</span>
</code></pre></div><pre><code>## # A tibble: 4,000 x 3
##    product    review                                                         score
##    &lt;chr&gt;      &lt;chr&gt;                                                          &lt;fct&gt;
##  1 B000J0LSBG &quot;this stuff is  not stuffing  its  not good at all  save your… other
##  2 B000EYLDYE &quot;I absolutely LOVE this dried fruit.  LOVE IT.  Whenever I ha… great
##  3 B0026LIO9A &quot;GREAT DEAL, CONVENIENT TOO.  Much cheaper than WalMart and I… great
##  4 B00473P8SK &quot;Great flavor, we go through a ton of this sauce! I discovere… great
##  5 B001SAWTNM &quot;This is excellent salsa/hot sauce, but you can get it for $2… great
##  6 B000FAG90U &quot;Again, this is the best dogfood out there.  One suggestion: … great
##  7 B006BXTCEK &quot;The box I received was filled with teas, hot chocolates, and… other
##  8 B002GWH5OY &quot;This is delicious coffee which compares favorably with much … great
##  9 B003R0MFYY &quot;Don't let these little tiny cans fool you.  They pack a lot … great
## 10 B001EO5ZXI &quot;One of the nicest, smoothest cup of chai I've made. Nice mix… great
## # … with 3,990 more rows
</code></pre><p>Computers, on the other hand, need that <code>review</code> variable to be heavily preprocessed and transformed in order for it to be ready for most modeling. We typically need to
<a href=https://smltar.com/tokenization.html target=_blank rel=noopener>tokenize</a> the text, find word frequencies, and perhaps
<a href=https://www.tidytextmining.com/tfidf.html target=_blank rel=noopener>compute tf-idf</a>. There are quite a number of different structures we can use to store the results of this preprocessing. We can keep the results in a long, tidy tibble, which is excellent for exploratory data analysis.</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=nf>library</span><span class=p>(</span><span class=n>tidytext</span><span class=p>)</span>

<span class=n>tidy_reviews</span> <span class=o>&lt;-</span> <span class=n>training_data</span> <span class=o>%&gt;%</span>
  <span class=nf>unnest_tokens</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>review</span><span class=p>)</span> <span class=o>%&gt;%</span>
  <span class=nf>count</span><span class=p>(</span><span class=n>product</span><span class=p>,</span> <span class=n>word</span><span class=p>)</span> <span class=o>%&gt;%</span>
  <span class=nf>bind_tf_idf</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>product</span><span class=p>,</span> <span class=n>n</span><span class=p>)</span>

<span class=n>tidy_reviews</span>
</code></pre></div><pre><code>## # A tibble: 208,306 x 6
##    product    word        n    tf   idf tf_idf
##    &lt;chr&gt;      &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1 B0000691JF and         1   0.1 0.234 0.0234
##  2 B0000691JF i           1   0.1 0.262 0.0262
##  3 B0000691JF in          1   0.1 0.654 0.0654
##  4 B0000691JF just        1   0.1 1.54  0.154 
##  5 B0000691JF manner      1   0.1 5.52  0.552 
##  6 B0000691JF ordered     1   0.1 2.76  0.276 
##  7 B0000691JF prompt      1   0.1 5.81  0.581 
##  8 B0000691JF the         1   0.1 0.206 0.0206
##  9 B0000691JF usual       1   0.1 5.04  0.504 
## 10 B0000691JF what        1   0.1 2.27  0.227 
## # … with 208,296 more rows
</code></pre><p>We can also transform these results to a wide format, often a good fit when the next step is a modeling or machine learning algorithm.</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=n>wide_reviews</span> <span class=o>&lt;-</span> <span class=n>tidy_reviews</span> <span class=o>%&gt;%</span>
  <span class=nf>select</span><span class=p>(</span><span class=n>product</span><span class=p>,</span> <span class=n>word</span><span class=p>,</span> <span class=n>tf_idf</span><span class=p>)</span> <span class=o>%&gt;%</span>
  <span class=nf>pivot_wider</span><span class=p>(</span><span class=n>names_from</span> <span class=o>=</span> <span class=n>word</span><span class=p>,</span> <span class=n>names_prefix</span> <span class=o>=</span> <span class=s>&#34;word_&#34;</span><span class=p>,</span>
              <span class=n>values_from</span> <span class=o>=</span> <span class=n>tf_idf</span><span class=p>,</span> <span class=n>values_fill</span> <span class=o>=</span> <span class=m>0</span><span class=p>)</span>

<span class=n>wide_reviews</span>
</code></pre></div><pre><code>## # A tibble: 4,000 x 13,797
##    product word_and  word_i word_in word_just word_manner word_ordered word_prompt
##    &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;
##  1 B00006…  0.0234  0.0262  0.0654     0.154        0.552       0.276        0.581
##  2 B00008…  0.00780 0       0          0            0           0            0    
##  3 B00008…  0.00177 0.00397 0.0198     0.0117       0           0            0    
##  4 B00008…  0.00582 0.00489 0.00813    0            0           0            0    
##  5 B00008…  0.00246 0.0166  0.0207     0.0162       0           0            0    
##  6 B00008…  0.00334 0.00750 0.00935    0            0           0            0    
##  7 B00008…  0.0114  0.00729 0.00909    0            0           0            0    
##  8 B00008…  0.00768 0.0129  0          0            0           0            0    
##  9 B00008…  0.00976 0       0          0            0           0            0    
## 10 B00008…  0.0156  0       0          0            0           0            0    
## 11 B00008…  0.00404 0.0181  0          0            0           0            0    
## 12 B00008…  0.0142  0.00397 0          0            0           0            0    
## 13 B00008…  0.0160  0.00596 0.0149     0.0351       0           0            0    
## 14 B00009…  0.00439 0.00656 0.00818    0            0           0            0    
## 15 B0000A…  0.00679 0.00380 0.0379     0            0           0.0401       0    
## # … with 3,985 more rows, and 13,789 more variables: word_the &lt;dbl&gt;,
## #   word_usual &lt;dbl&gt;, word_what &lt;dbl&gt;, word_a &lt;dbl&gt;, word_anymore &lt;dbl&gt;,
## #   word_chocolate &lt;dbl&gt;, word_coat &lt;dbl&gt;, word_dogfood &lt;dbl&gt;, word_ears &lt;dbl&gt;,
## #   word_fine &lt;dbl&gt;, word_for &lt;dbl&gt;, word_great &lt;dbl&gt;, word_hardly &lt;dbl&gt;,
## #   word_he &lt;dbl&gt;, word_health &lt;dbl&gt;, word_his &lt;dbl&gt;, word_hot &lt;dbl&gt;,
## #   word_is &lt;dbl&gt;, word_itching &lt;dbl&gt;, word_lab &lt;dbl&gt;, …
</code></pre><p>Lots of zeroes! Instead of using a tibble, we can transform these results to a <strong>sparse matrix</strong>, a specialized data structure that keeps track of only the non-zero elements instead of every element.</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=n>sparse_reviews</span> <span class=o>&lt;-</span> <span class=n>tidy_reviews</span> <span class=o>%&gt;%</span>
  <span class=nf>cast_dfm</span><span class=p>(</span><span class=n>product</span><span class=p>,</span> <span class=n>word</span><span class=p>,</span> <span class=n>tf_idf</span><span class=p>)</span>

<span class=n>sparse_reviews</span>
</code></pre></div><pre><code>## Document-feature matrix of: 4,000 documents, 13,796 features (99.6% sparse).
</code></pre><p>As is typical for text data, this document-feature matrix is extremely sparse, with many zeroes. Most documents do not contain most words. By using this kind of specialized structure instead of anything like a vanilla <code>matrix</code> or <code>data.frame</code>, we secure two benefits:</p><ul><li>We can taken advantage of the <strong>speed</strong> gained from any specialized model algorithms built for sparse data.</li><li>The amount of <strong>memory</strong> this object requires decreases dramatically.</li></ul><p>How big of a change in memory are we talking about?</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=n>lobstr</span><span class=o>::</span><span class=nf>obj_sizes</span><span class=p>(</span><span class=n>wide_reviews</span><span class=p>,</span> <span class=n>sparse_reviews</span><span class=p>)</span>
</code></pre></div><pre><code>## * 443,539,792 B
## *   3,581,200 B
</code></pre><h2 id=a-blueprint-for-sparse-models>A blueprint for sparse models
<a href=#a-blueprint-for-sparse-models><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>Before the most recent releases of hardhat, parsnip, and tune, there was no support for sparse data structures within tidymodels. Now, you can specify a hardhat <strong>blueprint</strong> for sparse data.</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=nf>library</span><span class=p>(</span><span class=n>hardhat</span><span class=p>)</span>
<span class=n>sparse_bp</span> <span class=o>&lt;-</span> <span class=nf>default_recipe_blueprint</span><span class=p>(</span><span class=n>composition</span> <span class=o>=</span> <span class=s>&#34;dgCMatrix&#34;</span><span class=p>)</span>
</code></pre></div><p>The <code>dgCMatrix</code> composition is from the
<a href="https://cran.r-project.org/package=Matrix" target=_blank rel=noopener>Matrix</a> package, and is the most standard class for sparse numeric matrices in modeling in R. (You can also specify a dense matrix composition with <code>composition = "matrix"</code>.)</p><h2 id=workflows-and-sparsity>Workflows and sparsity
<a href=#workflows-and-sparsity><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>The blueprint is used under the hood by the hardhat functions to process data. To get ready to fit our model using the sparse blueprint, we can set up our preprocessing recipe:</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=nf>library</span><span class=p>(</span><span class=n>textrecipes</span><span class=p>)</span>

<span class=n>text_rec</span> <span class=o>&lt;-</span>
  <span class=nf>recipe</span><span class=p>(</span><span class=n>score</span> <span class=o>~</span> <span class=n>review</span><span class=p>,</span> <span class=n>data</span> <span class=o>=</span> <span class=n>training_data</span><span class=p>)</span> <span class=o>%&gt;%</span>
  <span class=nf>step_tokenize</span><span class=p>(</span><span class=n>review</span><span class=p>)</span>  <span class=o>%&gt;%</span>
  <span class=nf>step_stopwords</span><span class=p>(</span><span class=n>review</span><span class=p>)</span> <span class=o>%&gt;%</span>
  <span class=nf>step_tokenfilter</span><span class=p>(</span><span class=n>review</span><span class=p>,</span> <span class=n>max_tokens</span> <span class=o>=</span> <span class=m>1e3</span><span class=p>)</span> <span class=o>%&gt;%</span>
  <span class=nf>step_tfidf</span><span class=p>(</span><span class=n>review</span><span class=p>)</span>
</code></pre></div><p>And we set up our model as we would normally:</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=n>lasso_spec</span> <span class=o>&lt;-</span>
  <span class=nf>logistic_reg</span><span class=p>(</span><span class=n>penalty</span> <span class=o>=</span> <span class=m>0.02</span><span class=p>,</span> <span class=n>mixture</span> <span class=o>=</span> <span class=m>1</span><span class=p>)</span> <span class=o>%&gt;%</span>
  <span class=nf>set_engine</span><span class=p>(</span><span class=s>&#34;glmnet&#34;</span><span class=p>)</span>
</code></pre></div><p>The regularized modeling of the glmnet package is an example of an algorithm that has specialized approaches for sparse data. If we pass in dense data with <code>set_engine("glmnet")</code>, the underlying model will take one approach, but it will use a different, faster approach especially built for sparse data if we pass in a sparse matrix. Typically, we would recommend centering and scaling predictors using <code>step_normalize()</code> before fitting a regularized model like glmnet. However, if we do this, we would no longer have all our zeroes and sparse data. Instead, we can &ldquo;normalize&rdquo; these text predictors using tf-idf so that they are all on the same scale.</p><p>Let&rsquo;s put together two workflows, one using the sparse blueprint and one using the default behavior.</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=n>wf_sparse</span> <span class=o>&lt;-</span> 
  <span class=nf>workflow</span><span class=p>()</span> <span class=o>%&gt;%</span>
  <span class=nf>add_recipe</span><span class=p>(</span><span class=n>text_rec</span><span class=p>,</span> <span class=n>blueprint</span> <span class=o>=</span> <span class=n>sparse_bp</span><span class=p>)</span> <span class=o>%&gt;%</span>
  <span class=nf>add_model</span><span class=p>(</span><span class=n>lasso_spec</span><span class=p>)</span>
  
<span class=n>wf_default</span> <span class=o>&lt;-</span> 
  <span class=nf>workflow</span><span class=p>()</span> <span class=o>%&gt;%</span>
  <span class=nf>add_recipe</span><span class=p>(</span><span class=n>text_rec</span><span class=p>)</span> <span class=o>%&gt;%</span>
  <span class=nf>add_model</span><span class=p>(</span><span class=n>lasso_spec</span><span class=p>)</span>
</code></pre></div><h2 id=comparing-model-results>Comparing model results
<a href=#comparing-model-results><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>Now let&rsquo;s use <code>fit_resamples()</code> to estimate how well this model fits with both options and measure performance for both.</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=nf>set.seed</span><span class=p>(</span><span class=m>123</span><span class=p>)</span>
<span class=n>food_folds</span> <span class=o>&lt;-</span> <span class=nf>vfold_cv</span><span class=p>(</span><span class=n>training_data</span><span class=p>,</span> <span class=n>v</span> <span class=o>=</span> <span class=m>3</span><span class=p>)</span>

<span class=n>results</span> <span class=o>&lt;-</span> <span class=n>bench</span><span class=o>::</span><span class=nf>mark</span><span class=p>(</span>
  <span class=n>iterations</span> <span class=o>=</span> <span class=m>10</span><span class=p>,</span> <span class=n>check</span> <span class=o>=</span> <span class=kc>FALSE</span><span class=p>,</span>
  <span class=n>sparse</span> <span class=o>=</span> <span class=nf>fit_resamples</span><span class=p>(</span><span class=n>wf_sparse</span><span class=p>,</span> <span class=n>food_folds</span><span class=p>),</span>  
  <span class=n>default</span> <span class=o>=</span> <span class=nf>fit_resamples</span><span class=p>(</span><span class=n>wf_default</span><span class=p>,</span> <span class=n>food_folds</span><span class=p>),</span> 
<span class=p>)</span>

<span class=n>results</span>
</code></pre></div><pre><code>## # A tibble: 2 x 6
##   expression      min   median `itr/sec` mem_alloc `gc/sec`
##   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;
## 1 sparse        7.78s    7.87s    0.127      788MB   0.127 
## 2 default       1.19m     1.2m    0.0139     870MB   0.0139
</code></pre><p>We see on the order of a 10x speed gain by using the sparse blueprint!</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=nf>autoplot</span><span class=p>(</span><span class=n>results</span><span class=p>,</span> <span class=n>type</span> <span class=o>=</span> <span class=s>&#34;ridge&#34;</span><span class=p>)</span>
</code></pre></div><p><img src=figure/unnamed-chunk-11-1.png alt="plot of chunk unnamed-chunk-11"></p><p>The model performance metrics are the same:</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=nf>fit_resamples</span><span class=p>(</span><span class=n>wf_sparse</span><span class=p>,</span> <span class=n>food_folds</span><span class=p>)</span> <span class=o>%&gt;%</span>
  <span class=nf>collect_metrics</span><span class=p>()</span>
</code></pre></div><pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.715     3 0.00399
## 2 roc_auc  binary     0.797     3 0.00598
</code></pre><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=nf>fit_resamples</span><span class=p>(</span><span class=n>wf_default</span><span class=p>,</span> <span class=n>food_folds</span><span class=p>)</span> <span class=o>%&gt;%</span>
  <span class=nf>collect_metrics</span><span class=p>()</span>
</code></pre></div><pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.715     3 0.00399
## 2 roc_auc  binary     0.797     3 0.00598
</code></pre><p>To see a detailed text modeling example using this dataset of food reviews, <em>without</em> sparse encodings but complete with tuning hyperparameters, check out
<a href=https://www.tidymodels.org/learn/work/tune-text/ target=_blank rel=noopener>our article on <code>tidymodels.org</code></a>.</p><h2 id=current-limits>Current limits
<a href=#current-limits><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>In tidymodels, the support for sparse data structures begins coming <em>out</em> of a
<a href=https://www.tmwr.org/recipes.html target=_blank rel=noopener>preprocessing recipe</a> and continues throughout the fitting and tuning process. We typically still expect the input <em>into</em> a recipe to be a data frame, as shown in this text analysis example, and there is very limited support within tidymodels for starting with a sparse matrix, for example by using <code>parsnip::fit_xy()</code>.</p><p>There are currently three models in parsnip that support a sparse data encoding:</p><ul><li>the glmnet engine for linear and logistic regression (including multinomial regression),</li><li>the XGBoost engine for boosted trees, and</li><li>the ranger engine for random forests.</li></ul><p>There is heterogeneity in how recipes themselves handle data internally; this is why we didn&rsquo;t see a huge decrease in memory use when comparing <code>wf_sparse</code> to <code>wf_default</code>. The
<a href=https://textrecipes.tidymodels.org/ target=_blank rel=noopener>textrecipes</a> package internally adopts the idea of a
<a href=https://textrecipes.tidymodels.org/reference/tokenlist.html target=_blank rel=noopener>tokenlist</a>, which is memory efficient for sparse data, but other recipe steps may handle data in a dense tibble structure. Keep these current limits in mind as you consider the memory requirements of your modeling projects!</p></div></div><div class=column25><div class="section hideOnMobile"><div class=sectionTitle>Contents</div><nav id=TableOfContents><ul><li><a href=#why-sparse-data>Why sparse data?</a></li><li><a href=#a-blueprint-for-sparse-models>A blueprint for sparse models</a></li><li><a href=#workflows-and-sparsity>Workflows and sparsity</a></li><li><a href=#comparing-model-results>Comparing model results</a></li><li><a href=#current-limits>Current limits</a></li></ul></nav></div><div class=section></div></div></div></div></div><div id=rStudioFooter class=band><div class=bandContent><div id=copyright>The tidyverse is proudly supported by <a class=rstudioLogo href=https://posit.co/ aria-label="Posit Homepage"></a></div><div id=logos><a href=https://www.tidyverse.org/google_privacy_policy>Privacy policy</a>
<a href=https://github.com/tidyverse class="footerLogo gitHub" aria-label="Tidyverse GitHub organization"></a><a href=https://twitter.com/hashtag/rstats class="footerLogo twitter" aria-label="rstats hashtag on twitter.com"></a></div></div></div></div></div><script src=/js/math-code.js></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script><script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type=application/javascript>var dnt=(navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack);var doNotTrack=(dnt=="1"||dnt=="yes");if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');if(window.sessionStorage){var GA_SESSION_STORAGE_KEY='ga:clientId';ga('create','UA-115082821-1',{'storage':'none','clientId':sessionStorage.getItem(GA_SESSION_STORAGE_KEY)});ga(function(tracker){sessionStorage.setItem(GA_SESSION_STORAGE_KEY,tracker.get('clientId'));});}
ga('set','anonymizeIp',true);ga('send','pageview');}</script></body></html></div></div></body></html>