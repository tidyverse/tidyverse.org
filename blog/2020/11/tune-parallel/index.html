<!doctype html><html><head><!doctype html><html lang=en-us><link rel=stylesheet href=/css/fonts.css><link rel=stylesheet href=/css/main-site.css><link rel=stylesheet href=/css/fa5-all.css><style type=text/css>body{background-color:#fff;color:#1a1917}a{color:#38577f}a:hover{color:#42709b}a:focus{outline-color:#42709b}.column25-left .sectionTitle a:hover:not(.current),.column16-left .sectionTitle a:hover:not(.current){color:#1a1917}.icon-attribution,.icon-attribution a{color:#1a1917;opacity:75%}#homeContent .band.first{background-color:#fff}#homeContent .band.second{background-color:#fdeba4}#homeContent .band.third{background-color:#fff}#rStudioHeader{background-color:#1a162d;color:#fff}#rStudioHeader .productName{color:#fff}#rStudioHeader .productName:hover,#rStudioHeader .productName:focus{color:#fdeba4}#rStudioHeader #menu .menuItem.a{background-color:#75aadb;color:#fff}#rStudioHeader #menu .menuItem:hover{background-color:#484557;color:#fff}#rStudioHeader #menu .menuItem.current{background-color:#fff;color:#1a162d;text-decoration:none}#rStudioFooter.band{background-color:#767381}#rStudioFooter .bandContent #copyright{color:#e3eef8}table tbody tr:nth-child(even){background-color:#f7faff}table tbody tr:nth-child(odd){background-color:#fff}.latest{border-top:.5em solid <no value>}.event{-moz-box-shadow:0 0 0 0 rgba(0,0,0,.1);-webkit-box-shadow:0 0 0 0 rgba(0,0,0,.1);box-shadow:0 0 0 0 rgba(0,0,0,.1)}.section .event{background-color:#eab0c41a}.section .event{border-top:7pt solid #a19ea936}.section .event a{color:#1a162d}.section .event{color:#404040}</style><link rel=stylesheet href=/css/tidyverse.css><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo 0.71.1"><title>Parallel processing with tune</title><meta property="og:title" content="Parallel processing with tune"><meta property="og:description" content="With version 0.1.2 of tune, there are more options for parallel processing."><meta property="og:type" content="article"><meta property="og:url" content="https://www.tidyverse.org/blog/2020/11/tune-parallel/"><meta property="twitter:card" content="summary_large_image"><meta property="og:title" content="Parallel processing with tune - Tidyverse"><meta property="description" content="With version 0.1.2 of tune, there are more options for parallel processing."><meta property="og:description" content="With version 0.1.2 of tune, there are more options for parallel processing."><meta property="og:image" content="https://www.tidyverse.org/blog/2020/11/tune-parallel/thumbnail-sq.jpg"><meta name=twitter:image content="https://www.tidyverse.org/blog/2020/11/tune-parallel/thumbnail-wd.jpg"><link rel=apple-touch-icon sizes=180x180 href=/images/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/images/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/images/favicons/favicon-16x16.png><link rel=manifest href=/images/favicons/site.webmanifest><link rel=mask-icon href=/images/favicons/safari-pinned-tab.svg><link rel="shortcut icon" href=/images/favicons/favicon.ico><meta name=msapplication-TileColor content="#da532c"><meta name=msapplication-config href=/images/favicons/browserconfig.xml><script type=text/javascript src=/js/jquery-3.5.1.min.js></script><script type=text/javascript src=/js/site.js></script><link rel=icon href=/images/favicon.ico><script type=text/javascript src=/js/clipboard.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin=anonymous></script><script defer data-domain=tidyverse.org,all.tidyverse.org src=https://plausible.io/js/plausible.js></script></head><body><div id=appTidyverseSite class="shrinkHeader alwaysShrinkHeader"><div id=main><div id=rStudioHeader><nav class=band><div class="innards bandContent"><div><a class=productName href=/%20/>Tidyverse</a></div><div id=menu><div id=menuToggler></div><div id=menuItems><a class=menuItem href=/packages/>Packages</a>
<a class="menuItem current" href=/blog/>Blog</a>
<a class=menuItem href=/learn/>Learn</a>
<a class=menuItem href=/help/>Help</a>
<a class=menuItem href=/contribute/>Contribute</a></div></div></div></nav></div><main><div class="band padForHeader pushFooter"><div class=bandContent><div class="full splitColumns withMobileMargins"><div class=column75><h1 class=article-title>Parallel processing with tune</h1><div class=article-header aria-hidden=true><div class=photo><img src=/blog/2020/11/tune-parallel/thumbnail-wd.jpg></div><div class=photoCredit>Photo by <a href=https://unsplash.com/photos/mJ35U595uhA>Joss Woodhead</a></div></div><span class=article-date><i class="fas fa-calendar-day fa-fw"></i>&nbsp;&nbsp;2020/11/27</span><p style=margin-bottom:.5em;margin-top:.85em><i class="fas fa-tags"></i>&nbsp;
<a href=/tags/parallelism/>parallelism</a>,
<a href=/tags/tidymodels/>tidymodels</a>,
<a href=/tags/tune/>tune</a></p><p style=margin-top:.5em><i class="fas fa-user-circle fa-fw"></i>&nbsp;
Max Kuhn</p><div class=article-content><p>This is the third post related to version 0.1.2 of the tune package. The
<a href=https://www.tidyverse.org/blog/2020/11/tune-0-1-2/ target=_blank rel=noopener>first post</a> discussed various new features while the
<a href=https://www.tidyverse.org/blog/2020/11/tidymodels-sparse-support/ target=_blank rel=noopener>second post</a> describes sparse matrix support. This post is an excerpt from an upcoming chapter in
<a href=https://www.tmwr.org/ target=_blank rel=noopener><em>Tidy Modeling with R</em></a> and is focused on parallel processing.</p><p>Previously, the tune package allowed for parallel processing of calculations in a few different places:</p><ul><li><p>Simple model resampling via <code>resample_fit()</code></p></li><li><p>Model tuning via <code>tune_grid()</code></p></li><li><p>During Bayesian optimization (<code>tune_bayes()</code>)</p></li></ul><p>In the new version of tune, there are more options related to how parallelism occurs. It&rsquo;s a little complicated and we&rsquo;ll start by describing the most basic method.</p><h2 id=parallelizing-the-resampling-loop>Parallelizing the resampling loop
<a href=#parallelizing-the-resampling-loop><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>For illustration, let&rsquo;s suppose that we are tuning a set of model parameters (e.g. not recipe parameters). In tidymodels, we always use
<a href=https://www.tmwr.org/resampling.html target=_blank rel=noopener>out-of-sample predictions to measure performance</a>. With grid search, pseudo-code that illustrates the computations are:</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=nf>for </span><span class=p>(</span><span class=n>resample</span> <span class=n>in</span> <span class=n>resamples</span><span class=p>)</span> <span class=p>{</span>
   <span class=c1># Create analysis and assessment sets</span>
   <span class=c1># Preprocess data (e.g. formula or recipe)</span>
   <span class=nf>for </span><span class=p>(</span><span class=n>model</span> <span class=n>in</span> <span class=n>configurations</span><span class=p>)</span> <span class=p>{</span>
      <span class=c1># Fit {model} to the {resample} analysis set</span>
      <span class=c1># Predict the {resample} assessment set</span>
   <span class=p>}</span>
<span class=p>}</span>
</code></pre></div><p>Prior to the new version of tune, the only option was to run the outer resampling loop in parallel. The inner modeling loop is run sequentially. The rationale for this was this: if you are doing any significant preprocessing of the data (e.g., a complex recipe), you only have to do that as many times as you have resamples. Since the model tuning is conditional on the preprocessed data, this is pretty computationally efficient.</p><p>There were two downsides to this approach:</p><ul><li><p>Suppose you have 10 resamples but access to 20 cores. The maximum core utilization would be 10 and using 10 cores might not maximize the computational efficiency.</p></li><li><p>Since tidymodels treats validation sets as a single resample, you can&rsquo;t parallel process at all.</p></li></ul><p>Parallel processing is somewhat unpredictable. While you might have a lot of cores (or machines) to throw at the problem, adding more might not help. This really depends on the model, the size of the data, and the parallel strategy used (i.e. forking vs socket).</p><p>To illustrate how this approach utilizes parallel workers, we&rsquo;ll use a case where there are 7 model tuning parameter values along with 5-fold cross-validation. This visualization shows how the tasks are allocated to the worker processes:</p><p><img src=figure/grid-logging-rs-1.svg title="plot of chunk grid-logging-rs" alt="plot of chunk grid-logging-rs" width=70%></p><p>The code assigns each of the five resamples to their own worker process which, in this case, is a core on a single desktop machine. That worker conducts the preprocessing then loops over the models. The preprocessing happens once per resample.</p><p>In the new version of tune, there is a control option called <code>parallel_over</code>. Setting this to a value of <code>"resamples"</code> will select this scheme to parallelize the computations.</p><h2 id=parallelizing-everything>Parallelizing everything
<a href=#parallelizing-everything><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>Another option that we can pursue is to take the two loops shown above and merge them into a single loop.</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span class=n>all_tasks</span> <span class=o>&lt;-</span> <span class=nf>crossing</span><span class=p>(</span><span class=n>resamples</span><span class=p>,</span> <span class=n>configurations</span><span class=p>)</span>

<span class=nf>for </span><span class=p>(</span><span class=n>iter</span> <span class=n>in</span> <span class=n>all_tasks</span><span class=p>)</span> <span class=p>{</span>                           
   <span class=c1># Create analysis and assessment sets for {iter}</span>
   <span class=c1># Preprocess data (e.g. formula or recipe)</span>
   <span class=c1># Fit model {iter} to the {iter} analysis set</span>
   <span class=c1># Predict the {iter} assessment set</span>
<span class=p>}</span>
</code></pre></div><p>With seven models and five resamples there are a total of 35 separate tasks that can be given to the worker processes. For this example, that would allow up to 35 cores/machines to run simultaneously. If we use a validation set, this would also enable the model loop to run in parallel.</p><p>The downside to this approach is that the preprocessing is unnecessarily repeated multiple times (depending on how tasks are allocated to the worker processes).</p><p>Taking our previous example, here is what the allocations look like if the 35 tasks are run across 10 cores:</p><p><img src=figure/grid-logging-all-1.svg alt="plot of chunk grid-logging-all"></p><p>For each resample, the preprocessing is needlessly run six additional times. If the preprocessing is fast, this might be the best approach.</p><p>To enable this approach, the control option is set to <code>parallel_over = "everything"</code>.</p><h2 id=automatic-strategy-detection>Automatic strategy detection
<a href=#automatic-strategy-detection><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>The default for <code>parallel_over</code> is <code>NULL</code>. This allows us to check and see if there are multiple resamples. If that is the case, it uses a value of <code>"resamples"</code>; otherwise, <code>"everything"</code> is used.</p><h2 id=how-much-faster-are-the-computations>How much faster are the computations?
<a href=#how-much-faster-are-the-computations><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>As an example, we tuned a boosted tree with the <code>xgboost</code> engine on a data set of 4,000 samples. Five-fold cross-validation was used with 10 candidate models. These data required some baseline preprocessing that did not require any estimation. The preprocessing was handled three different ways:</p><ol><li>Preprocess the data prior to modeling using a <code>dplyr</code> pipeline (labeled as &ldquo;none&rdquo; in the plots below).</li><li>Conduct the same preprocessing using a recipe (shown as &ldquo;light&rdquo; preprocessing).</li><li>With a recipe, add an additional step that has a high computational cost (labeled as &ldquo;expensive&rdquo;).</li></ol><p>The first and second preprocessing options are designed to measure the computational cost of the recipe. The third option measures the cost of performing redundant computations with <code>parallel_over = "everything"</code>.</p><p>We evaluated this process using variable number of worker processes and using the two <code>parallel_over</code> options. The computer has 10 physical cores and 20 virtual cores (via hyper threading).</p><p>Let&rsquo;s consider the raw execution times:</p><p><img src=figure/grid-par-times-1.svg alt="plot of chunk grid-par-times"></p><p>Since there were only five resamples, the number of cores used when <code>parallel_over = "resamples"</code> is limited to five.</p><p>Comparing the curves in the first two panels for &ldquo;none&rdquo; and &ldquo;light&rdquo;:</p><ul><li><p>There is little difference in the execution times between the panels. This indicates, for these data, there is no real computational penalty for doing the preprocessing steps in a recipe.</p></li><li><p>There is some benefit for using <code>parallel_over = "everything"</code> with many cores. However, as shown below, the majority of the benefit of parallel processing occurs in the first five workers.</p></li></ul><p>With the expensive preprocessing step, there is a considerable difference in execution times. Using <code>parallel_over = "everything"</code> is problematic since, even using all cores, it never achieves the execution time that <code>parallel_over = "resamples"</code> attains with five cores. This is because the costly preprocessing step is unnecessarily repeated in the computational scheme.</p><h2 id=psock-clusters>PSOCK clusters
<a href=#psock-clusters><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>The primary method for parallel processing on Windows computers uses a PSOCK cluster. From
<a href=https://www.oreilly.com/library/view/parallel-r/9781449317850/ target=_blank rel=noopener><em>Parallel R</em></a>:</p><blockquote><p>&ldquo;The parallel package comes with two transports: &lsquo;PSOCK&rsquo; and &lsquo;FORK&rsquo;. The &lsquo;PSOCK&rsquo; transport is a streamlined version of
<a href=https://biostats.bepress.com/uwbiostat/paper193/ target=_blank rel=noopener>snow</a>&lsquo;s &lsquo;SOCK&rsquo; transport. It starts workers using the Rscript command, and communicates between the master and workers using socket connections.&rdquo;</p></blockquote><p>This method works on all major operating systems.</p><p>Different parallel processing technologies work in different ways. About mid-year we started to receive a number of issue reports where PSOCK clusters were failing on Windows. This was due to how parallel workers are initialized; they really don&rsquo;t know anything about the main R process (e.g., what packages are loaded, what data objects should have access, etc). Those problems are now solved with the most recent versions of the parsnip, recipes, and tune packages.</p></div></div><div class=column25><div class="section hideOnMobile"><div class=sectionTitle>Contents</div><nav id=TableOfContents><ul><li><a href=#parallelizing-the-resampling-loop>Parallelizing the resampling loop</a></li><li><a href=#parallelizing-everything>Parallelizing everything</a></li><li><a href=#automatic-strategy-detection>Automatic strategy detection</a></li><li><a href=#how-much-faster-are-the-computations>How much faster are the computations?</a></li><li><a href=#psock-clusters>PSOCK clusters</a></li></ul></nav></div><div class=section></div></div></div></div></div><div id=rStudioFooter class=band><div class=bandContent><div id=copyright>The tidyverse is proudly supported by <a class=rstudioLogo href=https://posit.co/ aria-label="Posit Homepage"></a></div><div id=logos><a href=https://www.tidyverse.org/google_privacy_policy>Privacy policy</a>
<a href=https://github.com/tidyverse class="footerLogo gitHub" aria-label="Tidyverse GitHub organization"></a><a href=https://twitter.com/hashtag/rstats class="footerLogo twitter" aria-label="rstats hashtag on twitter.com"></a></div></div></div></div></div><script type=application/javascript>var dnt=(navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack);var doNotTrack=(dnt=="1"||dnt=="yes");if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');if(window.sessionStorage){var GA_SESSION_STORAGE_KEY='ga:clientId';ga('create','UA-115082821-1',{'storage':'none','clientId':sessionStorage.getItem(GA_SESSION_STORAGE_KEY)});ga(function(tracker){sessionStorage.setItem(GA_SESSION_STORAGE_KEY,tracker.get('clientId'));});}
ga('set','anonymizeIp',true);ga('send','pageview');}</script></body></html></div></div></body></html>