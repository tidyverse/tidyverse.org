<!doctype html><html><head><!doctype html><html lang=en-us><link rel=stylesheet href=/css/fonts.css><link rel=stylesheet href=/css/main-site.css><link rel=stylesheet href=/css/fa5-all.css><style type=text/css>body{background-color:#fff;color:#1a1917}a{color:#38577f}a:hover{color:#42709b}a:focus{outline-color:#42709b}.column25-left .sectionTitle a:hover:not(.current),.column16-left .sectionTitle a:hover:not(.current){color:#1a1917}.icon-attribution,.icon-attribution a{color:#1a1917;opacity:75%}#homeContent .band.first{background-color:#fff}#homeContent .band.second{background-color:#fdeba4}#homeContent .band.third{background-color:#fff}#rStudioHeader{background-color:#1a162d;color:#fff}#rStudioHeader .productName{color:#fff}#rStudioHeader .productName:hover,#rStudioHeader .productName:focus{color:#fdeba4}#rStudioHeader #menu .menuItem.a{background-color:#75aadb;color:#fff}#rStudioHeader #menu .menuItem:hover{background-color:#484557;color:#fff}#rStudioHeader #menu .menuItem.current{background-color:#fff;color:#1a162d;text-decoration:none}#rStudioFooter.band{background-color:#767381}#rStudioFooter .bandContent #copyright{color:#e3eef8}table tbody tr:nth-child(even){background-color:#f7faff}table tbody tr:nth-child(odd){background-color:#fff}.latest{border-top:.5em solid <no value>}.event{-moz-box-shadow:0 0 0 0 rgba(0,0,0,.1);-webkit-box-shadow:0 0 0 0 rgba(0,0,0,.1);box-shadow:0 0 0 0 rgba(0,0,0,.1)}.section .event{background-color:#eab0c41a}.section .event{border-top:7pt solid #a19ea936}.section .event a{color:#1a162d}.section .event{color:#404040}</style><link rel=stylesheet href=/css/tidyverse.css><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo 0.71.1"><title>Fair machine learning with tidymodels</title><meta property="og:title" content="Fair machine learning with tidymodels"><meta property="og:description" content="Recent tidymodels releases integrated a set of tools for assessing whether machine learning models treat groups of people differently."><meta property="og:type" content="article"><meta property="og:url" content="https://www.tidyverse.org/blog/2024/03/tidymodels-fairness/"><meta property="twitter:card" content="summary_large_image"><meta property="og:title" content="Fair machine learning with tidymodels - Tidyverse"><meta property="description" content="Recent tidymodels releases integrated a set of tools for assessing whether machine learning models treat groups of people differently."><meta property="og:description" content="Recent tidymodels releases integrated a set of tools for assessing whether machine learning models treat groups of people differently."><meta property="og:image" content="https://www.tidyverse.org/blog/2024/03/tidymodels-fairness/thumbnail-sq.jpg"><meta name=twitter:image content="https://www.tidyverse.org/blog/2024/03/tidymodels-fairness/thumbnail-wd.jpg"><link rel=apple-touch-icon sizes=180x180 href=/images/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/images/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/images/favicons/favicon-16x16.png><link rel=manifest href=/images/favicons/site.webmanifest><link rel=mask-icon href=/images/favicons/safari-pinned-tab.svg><link rel="shortcut icon" href=/images/favicons/favicon.ico><meta name=msapplication-TileColor content="#da532c"><meta name=msapplication-config href=/images/favicons/browserconfig.xml><script type=text/javascript src=/js/jquery-3.5.1.min.js></script><script type=text/javascript src=/js/site.js></script><link rel=icon href=/images/favicon.ico><script type=text/javascript src=/js/clipboard.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin=anonymous></script><script defer data-domain=tidyverse.org,all.tidyverse.org src=https://plausible.io/js/plausible.js></script></head><body><div id=appTidyverseSite class="shrinkHeader alwaysShrinkHeader"><div id=main><div id=rStudioHeader><nav class=band><div class="innards bandContent"><div><a class=productName href=/%20/>Tidyverse</a></div><div id=menu><div id=menuToggler></div><div id=menuItems><a class=menuItem href=/packages/>Packages</a>
<a class="menuItem current" href=/blog/>Blog</a>
<a class=menuItem href=/learn/>Learn</a>
<a class=menuItem href=/help/>Help</a>
<a class=menuItem href=/contribute/>Contribute</a></div></div></div></nav></div><main><div class="band padForHeader pushFooter"><div class=bandContent><div class="full splitColumns withMobileMargins"><div class=column75><h1 class=article-title>Fair machine learning with tidymodels</h1><div class=article-header aria-hidden=true><div class=photo><img src=/blog/2024/03/tidymodels-fairness/thumbnail-wd.jpg></div><div class=photoCredit>Photo by <a href=https://unsplash.com/photos/JBghIzjbuLs>Patrick Fore</a></div></div><span class=article-date><i class="fas fa-calendar-day fa-fw"></i>&nbsp;&nbsp;2024/03/21</span><p style=margin-bottom:.5em;margin-top:.85em><i class="fas fa-tags"></i>&nbsp;
<a href=/tags/tidymodels/>tidymodels</a>,
<a href=/tags/tune/>tune</a>,
<a href=/tags/yardstick/>yardstick</a></p><p style=margin-top:.5em><i class="fas fa-user-circle fa-fw"></i>&nbsp;
Simon Couch</p><div class=article-content><p>We&rsquo;re very, very excited to announce the introduction of tools for assessing model fairness in tidymodels. This effort involved coordination from various groups at Posit over the course of over a year and resulted in a toolkit that we believe is both principled and impactful.</p><p>Fairness assessment features for tidymodels extend across a number of packages; to install each, use the tidymodels meta-package:</p><div class=highlight><pre class=chroma><code class=language-r data-lang=r><span><span class=nf><a href=https://rdrr.io/r/utils/install.packages.html>install.packages</a></span><span class=o>(</span><span class=s>"tidymodels"</span><span class=o>)</span></span></code></pre></div><h2 id=machine-learning-fairness>Machine learning fairness
<a href=#machine-learning-fairness><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>In recent years, high-profile analyses have called attention to many contexts where the use of machine learning deepened inequities in our communities. In late 2022, a group of Posit employees across teams, roles, and technical backgrounds formed a reading group to engage with literature on machine learning fairness, a research field that aims to define what it means for a statistical model to act unfairly and take measures to address that unfairness. We then designed new software functionality and learning resources to help data scientists measure and critique the ways in which the machine learning models they&rsquo;ve built might disparately impact people affected by that model.</p><p>Perhaps the core question that fairness as a research field has tried to address is exactly what a machine learning model acting fairly entails. As a recent primer notes, &ldquo;[t]he rapid growth of this new field has led to wildly inconsistent motivations, terminology, and notation, presenting a serious challenge for cataloging and comparing definitions&rdquo; (Mitchell et al. 2021).</p><p>Broadly, approaches to fairness provide tooling&mdash;whether social or algorithmic&mdash;to understand the social implications of utilizing a machine learning model. Different researchers categorize approaches to fairness differently, but work in this area can be loosely summarized as falling into one or more of the following categories: assessment, mitigation, and critique.</p><ul><li><p><em>Assessment</em>: Fairness assessment tooling allows practitioners to measure the degree to which a machine learning model acts unfairly given some definition of fairness. The chosen definition of fairness greatly impacts whether a model&rsquo;s predictions are regarded as fair. While there have been many, many definitions of fairness proposed&mdash;a popular tutorial on these approaches compares 21 canonical definitions&mdash;most all of them involve simple inequalities based on a small set of conditional probabilities (Narayanan 2018; Mitchell et al. 2021).</p></li><li><p><em>Mitigation</em>: Given a fairness assessment, mitigation approaches reduce the degree to which a machine learning model acts unfairly given some definition of fairness. Making a model more fair according to one metric may make that model less fair according to another. Approaches to mitigation are subject to impossibility theorems, which show that &ldquo;definitions are not mathematically or morally compatible in general&rdquo; (Mitchell et al. 2021). That is, there is no way to satisfy many fairness constraints at once unless we live in a world with no inequality to start with. However, more recent studies have shown that near-fairness with respect to several definitions is quite possible (Bell et al. 2023).</p></li><li><p><em>Critique</em>: While approaches to assessment and mitigation seek to reduce complexity and situate notions of fairness in mathematical formalism, sociotechnical critique provides tooling to better understand how mathematical notions of fairness may fail to account for the real-world complexity of social phenomena. Work in this discipline often reveals that, in the process of measuring or addressing unfairness by some definition, methods for fairness assessment and mitigation may actually ignore, necessitate, or introduce unfairness by some other definition.</p></li></ul><p>The work of scoping Posit&rsquo;s resources for fair machine learning, in large part, involved striking the right balance between tools in these categories and integrating them thoughtfully among our existing functionality. Rather than supporting as many fairness-oriented tools as possible, our goal is to best enable users of our tools to reason well about the fairness-relevant decisions they make throughout the modeling process.</p><h2 id=additions-to-tidymodels>Additions to tidymodels
<a href=#additions-to-tidymodels><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><p>The most recent set of tidymodels releases include changes that provide support for assessment and critique using the tidymodels framework.</p><p>The most recent yardstick release introduces
<a href=https://yardstick.tidymodels.org/reference/new_groupwise_metric.html target=_blank rel=noopener>a tool to create fairness metrics</a> with the problem context in mind, as well as
<a href=https://yardstick.tidymodels.org/reference/index.html#fairness-metrics target=_blank rel=noopener>some outputs of that tool</a> implementing common fairness metrics. For a higher-level introduction to the concept of a groupwise metric, we&rsquo;ve also introduced a
<a href=https://yardstick.tidymodels.org/articles/grouping.html target=_blank rel=noopener>new package vignette</a>. To see those fairness metrics in action, see
<a href=https://www.tidymodels.org/learn/work/fairness-detectors/ target=_blank rel=noopener>this new article on tidymodels.org</a>, a case study using data about GPT detectors.</p><p>The most recent tune release integrates support for those fairness metrics from yardstick, allowing users to evaluate fairness criteria across resamples. To demonstrate those features in context, we&rsquo;ve added
<a href=https://www.tidymodels.org/learn/work/fairness-readmission/ target=_blank rel=noopener>another new article on tidymodels.org</a>, modeling hospital readmission for patients with Type I diabetes.</p><p>Notably, we haven&rsquo;t introduced functionality to support mitigation. While a number of methods have proliferated over the years to finetune models to act more fairly with respect to some fairness criteria, each apply only in relatively niche applications with modest experimental results (Agarwal et al. 2018; Mittelstadt, Wachter, and Russell 2023). For now, we believe that, in practice, the efforts of practitioners&mdash;and thus our efforts to support them&mdash;are better spent engaging with the sociotechnical context of a given modeling problem (Holstein et al. 2019).</p><p>We&rsquo;re excited to support modeling practitioners in fairness-oriented analysis of models and look forward to seeing how these methods are put to work.</p><h2 id=references>References
<a href=#references><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="currentcolor"/><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"/></svg></a></h2><div id=refs class="references csl-bib-body hanging-indent" entry-spacing=0><div id=ref-agarwal2018 class=csl-entry><p>Agarwal, Alekh, Alina Beygelzimer, Miroslav Dudı́k, John Langford, and Hanna Wallach. 2018. &ldquo;A Reductions Approach to Fair Classification.&rdquo; In <em>International Conference on Machine Learning</em>, 60&ndash;69. PMLR.</p></div><div id=ref-bell2023 class=csl-entry><p>Bell, Andrew, Lucius Bynum, Nazarii Drushchak, Tetiana Zakharchenko, Lucas Rosenblatt, and Julia Stoyanovich. 2023. &ldquo;The Possibility of Fairness: Revisiting the Impossibility Theorem in Practice.&rdquo; In <em>Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency</em>, 400&ndash;422. FAccT &lsquo;23. New York, NY, USA: Association for Computing Machinery. <a href=https://doi.org/10.1145/3593013.3594007>https://doi.org/10.1145/3593013.3594007</a>.</p></div><div id=ref-holstein2019 class=csl-entry><p>Holstein, Kenneth, Jennifer Wortman Vaughan, Hal Daumé III, Miro Dudik, and Hanna Wallach. 2019. &ldquo;Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?&rdquo; In <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>, 1&ndash;16.</p></div><div id=ref-mitchell2021 class=csl-entry><p>Mitchell, Shira, Eric Potash, Solon Barocas, Alexander D&rsquo;Amour, and Kristian Lum. 2021. &ldquo;Algorithmic Fairness: Choices, Assumptions, and Definitions.&rdquo; <em>Annual Review of Statistics and Its Application</em> 8 (1): 141&ndash;63. <a href=https://doi.org/10.1146/annurev-statistics-042720-125902>https://doi.org/10.1146/annurev-statistics-042720-125902</a>.</p></div><div id=ref-mittelstadt2023 class=csl-entry><p>Mittelstadt, Brent, Sandra Wachter, and Chris Russell. 2023. &ldquo;The Unfairness of Fair Machine Learning: Levelling down and Strict Egalitarianism by Default.&rdquo; <em>arXiv Preprint arXiv:2302.02404</em>.</p></div><div id=ref-narayanan2018 class=csl-entry><p>Narayanan, Arvind. 2018. &ldquo;Translation Tutorial: 21 Fairness Definitions and Their Politics.&rdquo; In <em>Proc. Conf. Fairness Accountability Transp., New York, Usa</em>, 1170:3.</p></div></div></div></div><div class=column25><div class="section hideOnMobile"><div class=sectionTitle>Contents</div><nav id=TableOfContents><ul><li><a href=#machine-learning-fairness>Machine learning fairness</a></li><li><a href=#additions-to-tidymodels>Additions to tidymodels</a></li><li><a href=#references>References</a></li></ul></nav></div><div class=section></div></div></div></div></div><div id=rStudioFooter class=band><div class=bandContent><div id=copyright>The tidyverse is proudly supported by <a class=rstudioLogo href=https://posit.co/ aria-label="Posit Homepage"></a></div><div id=logos><a href=https://www.tidyverse.org/google_privacy_policy>Privacy policy</a>
<a href=https://github.com/tidyverse class="footerLogo gitHub" aria-label="Tidyverse GitHub organization"></a><a href=https://twitter.com/hashtag/rstats class="footerLogo twitter" aria-label="rstats hashtag on twitter.com"></a></div></div></div></div></div><script type=application/javascript>var dnt=(navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack);var doNotTrack=(dnt=="1"||dnt=="yes");if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');if(window.sessionStorage){var GA_SESSION_STORAGE_KEY='ga:clientId';ga('create','UA-115082821-1',{'storage':'none','clientId':sessionStorage.getItem(GA_SESSION_STORAGE_KEY)});ga(function(tracker){sessionStorage.setItem(GA_SESSION_STORAGE_KEY,tracker.get('clientId'));});}
ga('set','anonymizeIp',true);ga('send','pageview');}</script></body></html></div></div></body></html>